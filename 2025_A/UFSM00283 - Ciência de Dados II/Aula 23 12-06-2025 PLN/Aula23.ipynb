{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução ao Processamento de Linguagem Natural (PLN) e Pré-processamento de Texto\n",
        "\n",
        "Este notebook tem como objetivo introduzir os conceitos fundamentais de Processamento de Linguagem Natural (PLN) e, em particular, as técnicas essenciais de pré-processamento de texto. O pré-processamento é uma etapa crucial em praticamente qualquer tarefa de PLN, pois transforma dados textuais brutos em um formato mais limpo e estruturado, adequado para análise e modelagem computacional.\n",
        "\n",
        "Vamos cobrir:\n",
        "1.  O que é PLN e por que o pré-processamento é importante.\n",
        "2.  Principais etapas do pré-processamento de texto.\n",
        "3.  Expressões Regulares (Regex)\n",
        "4.  Stemming (Radicalização) vs. Lematização.\n",
        "5.  Exemplos práticos de código utilizando a biblioteca NLTK e o módulo `re` em Python.\n",
        "6.  Exercícios práticos para aplicar os conhecimentos.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Nr8jxxCcECX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. O que é PLN e Por que Pré-processar Texto?\n",
        "\n",
        "### O que é PLN?\n",
        "\n",
        "**Processamento de Linguagem Natural (PLN)**, do inglês *Natural Language Processing (NLP)*, é um campo da inteligência artificial que se concentra na interação entre computadores e linguagens humanas (naturais). O objetivo é permitir que os computadores \"entendam\", \"interpretem\" e \"manipulem\" a linguagem humana.\n",
        "\n",
        "Aplicações de PLN incluem:\n",
        "*   Assistentes virtuais (Siri, Google Assistant, Alexa)\n",
        "*   Tradutores automáticos (Google Translate)\n",
        "*   Análise de Sentimento (entender a opinião em textos)\n",
        "*   Chatbots\n",
        "*   Sumarização automática de textos\n",
        "*   Sistemas de busca (Google Search)\n",
        "\n",
        "### Por que Pré-processar Texto?\n",
        "\n",
        "A linguagem humana é rica, complexa e muitas vezes ambígua. Textos brutos contêm ruídos, variações gramaticais, erros de digitação, pontuação e formatação que podem dificultar o processamento computacional.\n",
        "\n",
        "O **pré-processamento de texto** é o processo de limpeza e preparação dos dados textuais para que possam ser utilizados de forma eficaz em tarefas de PLN. É como limpar e organizar ingredientes antes de cozinhar; os resultados serão muito melhores!\n",
        "\n",
        "**Objetivos do pré-processamento:**\n",
        "*   Reduzir ruído e redundância.\n",
        "*   Padronizar o texto.\n",
        "*   Converter texto em um formato mais fácil de analisar (e.g., listas de palavras).\n",
        "*   Reduzir a dimensionalidade dos dados (número de palavras únicas).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "iiYpWHH3cm0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Principais Etapas do Pré-processamento de Texto\n",
        "\n",
        "Vamos explorar algumas das etapas mais comuns e importantes no pré-processamento de texto. Usaremos a biblioteca NLTK (Natural Language Toolkit) em Python, que é uma das ferramentas mais populares para PLN, e o módulo `re` para Expressões Regulares.\n",
        "\n",
        "Primeiro, precisamos instalar a biblioteca NLTK e baixar os recursos necessários."
      ],
      "metadata": {
        "id": "Mx7bWzFOc4Dn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZqmnERJOLSrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de06505a-5c7c-4d92-ffc9-140092c9d547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificando e baixando recursos NLTK...\n",
            "Recurso 'punkt' já baixado.\n",
            "Recurso 'stopwords' já baixado.\n",
            "Recurso 'rslp' já baixado.\n",
            "Recurso 'wordnet' não encontrado localmente. Baixando...\n",
            "Recurso 'averaged_perceptron_tagger' já baixado.\n",
            "Verificação e download de recursos NLTK concluídos!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "# Baixar recursos necessários do NLTK\n",
        "# punkt: para tokenização de sentenças e palavras\n",
        "# stopwords: lista de palavras comuns a serem removidas\n",
        "# rslp: stemmer para português\n",
        "# wordnet: dicionário para lematização (principalmente inglês)\n",
        "# averaged_perceptron_tagger: para POS tagging (usado na lematização)\n",
        "\n",
        "print(\"Verificando e baixando recursos NLTK...\")\n",
        "\n",
        "nltk_resources = ['punkt', 'stopwords', 'rslp', 'wordnet', 'averaged_perceptron_tagger']\n",
        "for resource in nltk_resources:\n",
        "    try:\n",
        "        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}' if resource in ['stopwords', 'wordnet'] else f'stemmers/{resource}' if resource == 'rslp' else f'taggers/{resource}')\n",
        "        print(f\"Recurso '{resource}' já baixado.\")\n",
        "    except LookupError:\n",
        "         print(f\"Recurso '{resource}' não encontrado localmente. Baixando...\")\n",
        "         nltk.download(resource)\n",
        "\n",
        "\n",
        "print(\"Verificação e download de recursos NLTK concluídos!\")\n",
        "\n",
        "# Carregar stop words do NLTK e stemmer (para uso posterior)\n",
        "stop_words_portugues = set(stopwords.words('portuguese'))\n",
        "stemmer_portugues = RSLPStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto_exemplo = \"Este É; 1 dos 30 eXEMplos - de TeXTO - COM 5 DIferentes... CAPitaliZAçÕES!\\n\""
      ],
      "metadata": {
        "id": "grNVC3MnhYwj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Converter para Minúsculas (Lowercasing)\n",
        "\n",
        "Converter todo o texto para minúsculas ajuda a garantir que palavras que diferem apenas na capitalização sejam tratadas como a mesma palavra (ex: \"Palavra\", \"palavra\", \"PALAVRA\")."
      ],
      "metadata": {
        "id": "NEY6uUgXdR-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_minusculo = texto_exemplo.lower()\n",
        "\n",
        "print(f\"Texto original: {texto_exemplo}\")\n",
        "print(f\"Texto em minúsculas: {texto_minusculo}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUaRpvxCeg36",
        "outputId": "6a5ff832-7259-4f71-bb79-17eda6876d32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Este É; 1 dos 30 eXEMplos - de TeXTO - COM 5 DIferentes... CAPitaliZAçÕES!\n",
            "\n",
            "Texto em minúsculas: este é; 1 dos 30 exemplos - de texto - com 5 diferentes... capitalizações!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##b) Remover Pontuação\n",
        "Pontuações (vírgulas, pontos, pontos de exclamação, etc.) geralmente não carregam significado semântico para a maioria das tarefas de PLN e podem ser removidas."
      ],
      "metadata": {
        "id": "V60UY_RHdgNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_despontuado = texto_minusculo.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "print(f\"Texto original: {texto_minusculo}\")\n",
        "print(f\"Texto sem pontuação: {texto_despontuado}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcUcgtp_ehsi",
        "outputId": "60906874-efc5-459f-9baa-842bd9c846aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: este é; 1 dos 30 exemplos - de texto - com 5 diferentes... capitalizações!\n",
            "\n",
            "Texto sem pontuação: este é 1 dos 30 exemplos  de texto  com 5 diferentes capitalizações\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c) Remover Números\n",
        "Dependendo da tarefa, números podem ser irrelevantes. Já mostramos um exemplo usando o módulo re (Expressões Regulares) para isso. Veremos mais sobre Regex em detalhes a seguir."
      ],
      "metadata": {
        "id": "zSkx2UQKd0RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A remoção de números (`re.sub(r'\\d+', '', text)`) é uma etapa comum em muitos pipelines de pré-processamento, mas a decisão de fazê-lo **depende totalmente da sua tarefa de PLN**.\n",
        "\n",
        "**Razões Comuns para Remover Números:**\n",
        "\n",
        "1.  **Reduzir Ruído e Dimensionalidade:** Para tarefas como análise de sentimento geral ou modelagem de tópicos, o valor numérico exato (como \"100\", \"2023\", \"5\") muitas vezes não contribui para o *significado principal* da frase. Um review que diz \"O produto custa 100 reais\" e outro que diz \"O produto custa 200 reais\" podem ter o mesmo sentimento (\"neutro\" ou \"positivo/negativo\" dependendo do contexto da frase). Tratar \"100\" e \"200\" como tokens separados pode inchar seu vocabulário sem adicionar valor semântico relevante para *essa tarefa específica*.\n",
        "2.  **Focar no Conteúdo Textual:** Muitos algoritmos de PLN focam na relação e frequência das palavras. Números podem quebrar padrões de texto (\"comprou em [NÚMERO] e gostou\") sem que o número em si seja o foco da análise.\n",
        "3.  **Padronização:** Em alguns casos, você pode querer saber *que* um número estava presente, mas não qual era. Substituir todos os números por um token especial (como `<NUM>`) padroniza essa informação e ainda reduz a dimensionalidade (em vez de infinitos números possíveis, você tem apenas um token `<NUM>`).\n",
        "\n",
        "**Quando NÃO Remover Números (ou Tratá-los Diferente):**\n",
        "\n",
        "Você **NÃO DEVE** remover números se a informação numérica for crucial para a tarefa de PLN. Exemplos:\n",
        "\n",
        "*   **Análise de Reviews de Produtos/Apps:** Se a tarefa for analisar *por que* um produto recebeu 1 estrela vs. 5 estrelas, o número na frase \"Recebi 1 estrela mas o produto é ótimo\" ou \"Dou 5 estrelas\" é *extremamente* importante.\n",
        "*   **Análise Financeira:** Valores monetários (\"lucro de 1 milhão\"), percentuais, datas, etc., são o cerne da análise.\n",
        "*   **Extração de Informação:** Se você quer extrair endereços, números de telefone, datas de validade, versões de software (\"versão 3.0\"), remover números inviabilizaria a tarefa.\n",
        "*   **Análise de Textos Científicos/Técnicos:** Medições, fórmulas, códigos de produtos, etc., são essenciais."
      ],
      "metadata": {
        "id": "iJbA25GVmxSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_desnumerado = re.sub(r'\\d+', '', texto_despontuado)\n",
        "# r'\\d+' <- padrão de expressão regular para busca de numeros\n",
        "\n",
        "print(f\"Texto original: {texto_despontuado}\")\n",
        "print(f\"Texto sem números: {texto_desnumerado}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7WbBe1yfkMC",
        "outputId": "dcac6b48-7dcb-4a2f-e3f0-65589b244a26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: este é 1 dos 30 exemplos  de texto  com 5 diferentes capitalizações\n",
            "\n",
            "Texto sem números: este é  dos  exemplos  de texto  com  diferentes capitalizações\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d) Remover Espaços em Branco Extras\n",
        "Múltiplos espaços entre palavras ou espaços no início/fim da string podem ser limpos. Uma forma simples é usar ``split()`` e ``join()``. Outra forma muito eficiente é usar Expressões Regulares, como veremos adiante."
      ],
      "metadata": {
        "id": "j6UPlGJSeCQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_sem_espacos = ' '.join(texto_desnumerado.split())\n",
        "\n",
        "print(f\"Texto original: {texto_desnumerado}\")\n",
        "print(f\"Texto sem espaços extras: {texto_sem_espacos}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUdTHbw2flaq",
        "outputId": "fe3e0639-f83e-4535-aacf-80672fb1a0fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: este é  dos  exemplos  de texto  com  diferentes capitalizações\n",
            "\n",
            "Texto sem espaços extras: este é dos exemplos de texto com diferentes capitalizações\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 3. Expressões Regulares (Regex) como aliadas no pré-processamento\n",
        "\n",
        "**Expressões Regulares (Regex ou Regexp)** são sequências de caracteres que definem um *padrão de busca*. São incrivelmente poderosas para encontrar, substituir ou extrair padrões complexos em texto de forma concisa. Embora a sintaxe possa parecer intimidadora no início, dominar o básico de Regex é um superpoder no processamento de texto.\n",
        "\n",
        "Em Python, o módulo `re` oferece suporte nativo para Expressões Regulares.\n",
        "\n",
        "### Por que usar Regex em PLN?\n",
        "\n",
        "*   **Limpeza de Ruído:** Remover padrões indesejados como tags HTML, URLs, emails, caracteres especiais, múltiplos espaços.\n",
        "*   **Extração de Informação:** Encontrar números de telefone, datas, menções de usuários, hashtags, etc.\n",
        "*   **Validação de Formato:** Verificar se uma string segue um formato específico (ex: formato de email, CPF).\n",
        "*   **Substituição:** Substituir padrões encontrados por outro texto (como fizemos para remover números).\n",
        "\n",
        "### Sintaxe Básica de Regex\n",
        "\n",
        "Aqui estão alguns dos elementos de padrão mais comuns:\n",
        "\n",
        "*   `a`, `1`, `@` - Caracteres Literais: correspondem exatamente a eles mesmos.\n",
        "*   `.` (Ponto): Corresponde a qualquer caractere (exceto quebra de linha `\\n`).\n",
        "*   `*` (Asterisco): Corresponde a **zero ou mais** ocorrências do elemento anterior. Ex: `a*` corresponde a \"\", \"a\", \"aa\", \"aaa\"...\n",
        "*   `+` (Mais): Corresponde a **uma ou mais** ocorrências do elemento anterior. Ex: `a+` corresponde a \"a\", \"aa\", \"aaa\"... mas não \"\".\n",
        "*   `?` (Interrogação): Corresponde a **zero ou uma** ocorrência do elemento anterior. Ex: `a?` corresponde a \"\" ou \"a\".\n",
        "*   `|` (Barra Vertical): Operador **OR**. Ex: `gato|cachorro` corresponde a \"gato\" ou \"cachorro\".\n",
        "*   `()` (Parênteses): Agrupamento e Captura. Permite aplicar operadores (`*`, `+`, `?`) a um grupo inteiro e capturar partes da string.\n",
        "*   `[]` (Colchetes): Conjunto de Caracteres. Corresponde a **qualquer um** dos caracteres dentro dos colchetes. Ex: `[aeiou]` corresponde a qualquer vogal minúscula. `[0-9]` corresponde a qualquer dígito. `[a-zA-Z]` corresponde a qualquer letra (maiúscula ou minúscula).\n",
        "*   `-` (Hífen) dentro de `[]`: Define um Intervalo. Ex: `[a-z]` (letras minúsculas de a a z), `[0-9]` (dígitos de 0 a 9).\n",
        "*   `^` (Circunflexo) dentro de `[]`: Negação. Corresponde a **qualquer caractere que NÃO ESTÁ** no conjunto. Ex: `[^0-9]` corresponde a qualquer caractere que não seja um dígito.\n",
        "*   `\\` (Barra Invertida): Caractere de Escape. Usado para \"escapar\" caracteres especiais (como `.`, `*`, `+`, `?`, `|`, `(`, `)`, `[`, `]`, `{`, `}`, `^`, `$`, `\\` ) ou para introduzir sequências especiais.\n",
        "    *   `\\d`: Corresponde a um dígito (`[0-9]`).\n",
        "    *   `\\D`: Corresponde a um não-dígito (`[^0-9]`).\n",
        "    *   `\\w`: Corresponde a um caractere de \"palavra\" (`[a-zA-Z0-9_]`).\n",
        "    *   `\\W`: Corresponde a um não-caractere de \"palavra\" (`[^a-zA-Z0-9_]`).\n",
        "    *   `\\s`: Corresponde a um caractere de espaço em branco (espaço, tab `\\t`, quebra de linha `\\n`, retorno de carro `\\r`, feed de formulário `\\f`, tab vertical `\\v`).\n",
        "    *   `\\S`: Corresponde a um não-caractere de espaço em branco.\n",
        "*   `^` (Circunflexo) fora de `[]`: Corresponde ao **início** da string.\n",
        "*   `$` (Cifrão): Corresponde ao **fim** da string.\n",
        "\n",
        "\n",
        "\n",
        "**Strings Raw (Brutas) em Python (`r\"...\"`)**: É uma boa prática usar strings raw para padrões Regex em Python (prefixo `r`). Isso impede que as barras invertidas (`\\`) sejam interpretadas pelo Python como sequências de escape de string Python (ex: `\\n` para nova linha) antes de serem passadas para o motor de Regex.\n",
        "\n",
        "### Funções do Módulo `re` (Principais para Pré-processamento)\n",
        "\n",
        "*   `re.search(pattern, string)`: Procura pela *primeira* ocorrência do padrão na string. Retorna um objeto `Match` se encontrar, `None` caso contrário.\n",
        "*   `re.findall(pattern, string)`: Encontra *todas* as ocorrências não sobrepostas do padrão na string e retorna uma **lista** de strings correspondentes.\n",
        "*   `re.sub(pattern, repl, string)`: Substitui todas as ocorrências do `pattern` encontrado na `string` pela string `repl`. É a função mais usada para limpeza.\n",
        "\n",
        "Vamos ver alguns exemplos práticos:"
      ],
      "metadata": {
        "id": "gpz8BX_2ecvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_regex = \"Meu email é teste@example.com e meu site é https://www.site.org.    Meu telefone é (11) 98765-4321.    Há também números 12345 e símbolos !!!\"\n",
        "\n",
        "print(f\"Texto original: {texto_regex}\\n\")"
      ],
      "metadata": {
        "id": "Fk0Dkx3IfX-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7798540-5213-4571-b882-3e68ff450453"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Meu email é teste@example.com e meu site é https://www.site.org.    Meu telefone é (11) 98765-4321.    Há também números 12345 e símbolos !!!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 1: Encontrar todos os números\n",
        "\n",
        "print(\"Números encontrados:\")\n",
        "numeros = re.findall(r'\\d+', texto_regex)\n",
        "print(numeros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJjYYB18iNFz",
        "outputId": "f3a0fe67-75ef-47b2-939e-a5d3bce8f65b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Números encontrados:\n",
            "['11', '98765', '4321', '12345']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 2: Substituir todos os espaços em branco múltiplos por um único espaço\n",
        "# \\s+ significa \"um ou mais caracteres de espaço em branco\"\n",
        "\n",
        "print(\"Texto com espaços em branco substituídos:\")\n",
        "texto_sem_espaco = re.sub(r'\\s+', ' ', texto_regex)\n",
        "print(texto_sem_espaco)"
      ],
      "metadata": {
        "id": "Uz-X0rr9f7CM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98e9510-978d-4cf2-c6bf-ae200e4b99e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto com espaços em branco substituídos:\n",
            "Meu email é teste@example.com e meu site é https://www.site.org. Meu telefone é (11) 98765-4321. Há também números 12345 e símbolos !!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 3: Remover caracteres que não são letras, números ou espaços\n",
        "# [^\\w\\s] significa \"qualquer caractere que NÃO SEJA (\\^) um caractere de palavra (\\w) OU (\\s) um espaço em branco\"\n",
        "# Poderíamos substituir por um espaço ou uma string vazia\n",
        "texto_com_especiais = \"Texto! Com? Simbolos@ E. Pontuacao,\"\n",
        "\n",
        "print(\"Texto com caracteres especiais removidos:\")\n",
        "texto_sem_especiais = re.sub(r'[^\\w\\s]', '', texto_com_especiais)\n",
        "print(texto_sem_especiais)"
      ],
      "metadata": {
        "id": "uxU_MgJ_f7bi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a97cd5-40eb-4b8f-bcf0-f42b174807a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto com caracteres especiais removidos:\n",
            "Texto Com Simbolos E Pontuacao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo 4: Remover URLs (exemplo comum em reviews online)\n",
        "texto_com_url = \"Produto bom! Veja em: http://loja.com/produto1 ou https://outraloja.org\"\n",
        "\n",
        "print(\"Texto com URLs removidas:\")\n",
        "texto_sem_url = re.sub(r'https?://\\S+', '', texto_com_url)\n",
        "print(texto_sem_url)"
      ],
      "metadata": {
        "id": "b1yeZx9Df8pl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa14faf7-35cd-4253-f054-57b8358ac7b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto com URLs removidas:\n",
            "Produto bom! Veja em:  ou \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como visto, Regex é uma ferramenta flexível que pode complementar ou substituir algumas das etapas de limpeza manuais que vimos anteriormente. No nosso pipeline de pré-processamento (seção 2), já usamos `re.sub` para remover números. Poderíamos usá-la também para remover pontuação ou limpar espaços."
      ],
      "metadata": {
        "id": "LqeGGrARgLU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Continuação das Etapas de Pré-processamento\n",
        "\n",
        "Agora que temos uma ferramenta poderosa para limpeza de padrões, vamos continuar com as etapas mais focadas na estrutura e significado das palavras.\n",
        "\n",
        "### e) Tokenização\n",
        "\n",
        "Tokenização é o processo de dividir uma string de texto em pedaços menores chamados **tokens**. Os tokens podem ser palavras, sub-palavras, ou até mesmo frases, dependendo da granularidade desejada. Geralmente, tokenizamos em palavras."
      ],
      "metadata": {
        "id": "qoR0A3aHgnKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "P4xoN8BBPVgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f05a0ba-423a-42a3-ad69-2132457376eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto_para_tokenizar = \"Tokenização é o processo de dividir texto em palavras.\"\n",
        "tokens = word_tokenize(texto_para_tokenizar, language='portuguese')\n",
        "\n",
        "print(f\"Texto original: {texto_para_tokenizar}\")\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "id": "l0Rq9XXmgL-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e22103d6-796f-4484-8db8-4e9c71f14d7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Tokenização é o processo de dividir texto em palavras.\n",
            "Tokens: ['Tokenização', 'é', 'o', 'processo', 'de', 'dividir', 'texto', 'em', 'palavras', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### f) Remover Stop Words\n",
        "\n",
        "**Stop words** são palavras muito comuns em uma língua que geralmente não adicionam muito significado a uma frase (ex: \"de\", \"a\", \"o\", \"e\", \"é\", \"em\", \"para\", etc.). Removê-las pode reduzir o tamanho do vocabulário e melhorar o desempenho em algumas tarefas.\n",
        "\n",
        "O NLTK possui listas de stop words para vários idiomas."
      ],
      "metadata": {
        "id": "zTteGuExgzwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_com_stopwords = \"Esta é uma frase de exemplo com muitas palavras comuns e irrelevantes.\"\n",
        "tokens = word_tokenize(texto_com_stopwords, language='portuguese')\n",
        "\n",
        "# Convertendo tokens para minúsculas ANTES de verificar se são stop words\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stop_words_portugues]\n",
        "\n",
        "print(f\"Tokens originais: {tokens}\")\n",
        "print(f\"Stop words em português (exemplo): {list(stop_words_portugues)[:10]}...\") # Mostrar apenas as primeiras 10\n",
        "print(f\"Tokens sem stop words: {tokens_sem_stopwords}\")"
      ],
      "metadata": {
        "id": "SVwkF4Rig1aV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8ee094-f4ad-4913-c455-2585f606c07e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens originais: ['Esta', 'é', 'uma', 'frase', 'de', 'exemplo', 'com', 'muitas', 'palavras', 'comuns', 'e', 'irrelevantes', '.']\n",
            "Stop words em português (exemplo): ['pelas', 'um', 'éramos', 'meu', 'só', 'seus', 'esta', 'estas', 'houvemos', 'teriam']...\n",
            "Tokens sem stop words: ['frase', 'exemplo', 'muitas', 'palavras', 'comuns', 'irrelevantes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Stemming (Radicalização) vs. Lematização\n",
        "\n",
        "Essas duas técnicas têm um objetivo semelhante: reduzir palavras flexionadas ou derivadas para uma forma base. No entanto, a abordagem e o resultado são diferentes.\n",
        "\n",
        "### Stemming (Radicalização)\n",
        "\n",
        "**Stemming** é um processo heurístico que corta sufixos (e às vezes prefixos) das palavras para chegar a um \"radical\" (stem). O radical resultante **não necessariamente é uma palavra real** da língua. É um processo mais rápido e menos preciso, útil para agrupar palavras com significados semelhantes.\n",
        "\n",
        "Exemplo em inglês: \"running\", \"runs\", \"ran\" podem ser reduzidos para o radical \"run\". \"beautiful\", \"beauty\" podem ser reduzidos para \"beauti\".\n",
        "\n",
        "Para português, o NLTK oferece o RSLP Stemmer (Remoção de Sufixos da Língua Portuguesa)."
      ],
      "metadata": {
        "id": "ExwuMA-XhAxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stemmer_portugues já carregado no início\n",
        "\n",
        "palavras = [\"correndo\", \"correram\", \"corre\", \"bonito\", \"beleza\", \"mulheres\", \"mulher\", \"linguagem\", \"linguagens\"]\n",
        "\n",
        "\n",
        "# Note que alguns radicais (\"corr\", \"mulher\") são palavras reais,\n",
        "# mas outros (\"bonit\", \"linguag\") não são necessariamente."
      ],
      "metadata": {
        "id": "p8JkSF8phFSP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lematização** é um processo mais sofisticado que utiliza vocabulário (um dicionário de palavras e suas formas) e análise morfológica para reduzir palavras flexionadas à sua forma base ou dicionário, conhecida como **lema**. O lema resultante **sempre é uma palavra real** da língua. É um processo mais lento, mas geralmente mais preciso que o stemming.\n",
        "\n",
        "Exemplo em inglês: \"running\", \"runs\", \"ran\" são lematizados para o lema \"run\". \"better\" é lematizado para \"good\". \"am\", \"is\", \"are\" são lematizados para \"be\".\n",
        "\n",
        "O NLTK possui o `WordNetLemmatizer`, mas ele funciona melhor para inglês e requer a especificação da Parte da Fala (POS tag) da palavra para um resultado mais preciso. Lematização robusta para português geralmente requer bibliotecas mais avançadas como spaCy ou outras ferramentas específicas para português, pois o NLTK WordNet não cobre bem o vocabulário português.\n",
        "\n",
        "Vamos demonstrar o conceito com o lemmatizer do NLTK para inglês e discutir o caso do português."
      ],
      "metadata": {
        "id": "kpRYreybhTbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mUh3FOkSQym",
        "outputId": "e2b51587-42ac-44e2-de7f-6db7d475b2df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# Para lematizar corretamente, muitas vezes precisamos do POS tag (Parte da Fala)\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Função auxiliar para obter o POS tag no formato que o WordNetLemmatizer entende\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Converte o POS tag do NLTK no formato do WordNet\"\"\"\n",
        "    if tag.startswith('J'): # Adjetivo\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'): # Verbo\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'): # Substantivo\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'): # Advérbio\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN # Padrão para substantivo"
      ],
      "metadata": {
        "id": "Ab-m87lehdeX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo em Inglês (para ilustrar o conceito com WordNetLemmatizer)\n",
        "palavras_en = [\"running\", \"runs\", \"ran\", \"better\", \"geese\", \"children\", \"is\", \"are\"]\n",
        "\n",
        "# Obter POS tags para cada palavra (simplificado para demonstração)\n",
        "# Em um pipeline real, processaríamos a frase inteira com um POS tagger\n",
        "pos_tags_en = nltk.pos_tag(palavras_en)\n",
        "\n",
        "lemas_en = [\n",
        "    lemmatizer.lemmatize(palavra, pos=get_wordnet_pos(tag))\n",
        "    for palavra, tag in pos_tags_en\n",
        "]\n",
        "\n",
        "print(f\"Palavras originais (Inglês): {palavras_en}\")\n",
        "print(f\"POS Tags (Inglês - simplificado): {pos_tags_en}\")\n",
        "print(f\"Lemas (Inglês): {lemas_en}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGQINFUjhesk",
        "outputId": "a2ec16fb-2a4f-4b4b-c07e-3ab415e17886"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras originais (Inglês): ['running', 'runs', 'ran', 'better', 'geese', 'children', 'is', 'are']\n",
            "POS Tags (Inglês - simplificado): [('running', 'VBG'), ('runs', 'NNS'), ('ran', 'VBD'), ('better', 'JJR'), ('geese', 'JJ'), ('children', 'NNS'), ('is', 'VBZ'), ('are', 'VBP')]\n",
            "Lemas (Inglês): ['run', 'run', 'run', 'good', 'geese', 'child', 'be', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Lematização em Português ---\")\n",
        "print(\"Para português, WordNetLemmatizer do NLTK não é adequado, pois não cobre o vocabulário.\")\n",
        "print(\"Bibliotecas como spaCy (com modelo pt) ou recursos específicos para português são necessários.\")\n",
        "print(\"Exemplo conceitual de lemas esperados para português:\")\n",
        "palavras_pt = [\"andando\", \"andaram\", \"anda\", \"feliz\", \"felicidade\", \"bonita\", \"bonitas\"]\n",
        "# Lema para \"andando\", \"andaram\", \"anda\" seria \"andar\"\n",
        "# Lema para \"feliz\", \"felicidade\" seria \"feliz\"\n",
        "# Lema para \"bonita\", \"bonitas\" seria \"bonito\" (ou \"bonita\" dependendo da convenção/recurso)\n",
        "\n",
        "print(f\"Palavras originais (Português): {palavras_pt}\")\n",
        "print(\"Lemas esperados (Conceito): ['andar', 'andar', 'andar', 'feliz', 'feliz', 'bonito', 'bonito']\")\n",
        "\n",
        "# Comparação com Stemming RSLP\n",
        "# stemmer_portugues já carregado no início\n",
        "radicais_pt = [stemmer_portugues.stem(palavra) for palavra in palavras_pt]\n",
        "print(f\"Radicais (Stemming RSLP): {radicais_pt}\")\n",
        "\n",
        "# Note a diferença: stemming produz radicais que podem não ser palavras reais (\"and\", \"bonit\"),\n",
        "# enquanto a lematização busca a forma do dicionário (\"andar\", \"feliz\", \"bonito\")."
      ],
      "metadata": {
        "id": "FLGryQSPhquB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af1be81-da76-420c-b68f-a1bf607e585d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Lematização em Português ---\n",
            "Para português, WordNetLemmatizer do NLTK não é adequado, pois não cobre o vocabulário.\n",
            "Bibliotecas como spaCy (com modelo pt) ou recursos específicos para português são necessários.\n",
            "Exemplo conceitual de lemas esperados para português:\n",
            "Palavras originais (Português): ['andando', 'andaram', 'anda', 'feliz', 'felicidade', 'bonita', 'bonitas']\n",
            "Lemas esperados (Conceito): ['andar', 'andar', 'andar', 'feliz', 'feliz', 'bonito', 'bonito']\n",
            "Radicais (Stemming RSLP): ['and', 'and', 'and', 'feliz', 'felic', 'bonit', 'bonit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quando usar Stemming vs. Lematização?**\n",
        "\n",
        "*   **Stemming:** Mais rápido, bom para tarefas onde a velocidade é importante e não é essencial que o resultado seja uma palavra real (ex: sistemas de busca simples onde você quer agrupar variações de uma palavra).\n",
        "*   **Lematização:** Mais lento, mais preciso, bom para tarefas que exigem um entendimento mais profundo do significado da palavra ou onde a forma real da palavra é importante (ex: análise de sentimento, tradução automática, construção de vocabulário para modelos complexos).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5_DUnWh7h34P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Pipeline Completo de Pré-processamento\n",
        "\n",
        "Vamos juntar várias dessas etapas em uma função para processar um texto de exemplo. Incorporaremos o uso de Regex na limpeza."
      ],
      "metadata": {
        "id": "HBKbNJQxiGU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, remove_numbers=True, remove_punctuation=True,\n",
        "                      remove_stopwords=True, apply_stemming=False):\n",
        "    \"\"\"\n",
        "    Aplica um pipeline de pré-processamento a um texto usando NLTK e Regex.\n",
        "\n",
        "    Args:\n",
        "        text (str): O texto de entrada.\n",
        "        remove_numbers (bool): Se True, remove números usando Regex.\n",
        "        remove_punctuation (bool): Se True, remove pontuação usando string.punctuation.\n",
        "        remove_stopwords (bool): Se True, remove stop words (do NLTK).\n",
        "        apply_stemming (bool): Se True, aplica stemming RSLP (do NLTK).\n",
        "\n",
        "    Returns:\n",
        "        list: Uma lista de tokens pré-processados.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return [] # Retorna lista vazia para entradas não string\n",
        "\n",
        "    # 1. Converter para minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remover URLs, mentions, hashtags (exemplos comuns com Regex)\n",
        "    text = re.sub(r'https?://\\S+', '', text) # Remove URLs (http, https)\n",
        "    text = re.sub(r'@\\w+', '', text) # Remove mentions (@usuario)\n",
        "    text = re.sub(r'#\\w+', '', text) # Remove hashtags (#hashtag)\n",
        "\n",
        "\n",
        "    # 3. Remover números (se solicitado) usando Regex\n",
        "    if remove_numbers:\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 4. Remover pontuação (se solicitado) usando string.punctuation\n",
        "    if remove_punctuation:\n",
        "         text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 5. Remover espaços em branco extras usando Regex (e strip)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Certificar-se de que o texto não está vazio após a limpeza inicial\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # 6. Tokenização (NLTK)\n",
        "    tokens = word_tokenize(text, language='portuguese')\n",
        "\n",
        "    # 7. Remover stop words (se solicitado - NLTK)\n",
        "    if remove_stopwords:\n",
        "        tokens = [palavra for palavra in tokens if palavra not in stop_words_portugues]\n",
        "\n",
        "    # 8. Aplicar stemming (se solicitado - NLTK)\n",
        "    if apply_stemming:\n",
        "        tokens = [stemmer_portugues.stem(palavra) for palavra in tokens]\n",
        "\n",
        "    # Opcional: Remover tokens vazios ou de 1 caractere que podem ter sobrado\n",
        "    tokens = [token for token in tokens if token and len(token) > 1]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "nUOBL-o4iLtx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso da função\n",
        "texto_completo_exemplo = \"Olá mundo! Este é um teste completo para ver como o pré-processamento funciona com PALAVRAS, números 123 e pontuações diferentes! Estou testando o stemming e a lematização. Site: http://teste.com.br. Curti muito o app! #top @usuario\"\n",
        "\n",
        "print(f\"Texto original: {texto_completo_exemplo}\\n\")\n",
        "\n",
        "# --- Processamento com NLTK (sem stemming) ---\n",
        "tokens_nltk_basico = preprocess_text(texto_completo_exemplo)\n",
        "print(f\"Tokens (NLTK): {tokens_nltk_basico}\")"
      ],
      "metadata": {
        "id": "fn9YVSRTiZCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994c3825-6152-473c-aa94-edd25d4be0a1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Olá mundo! Este é um teste completo para ver como o pré-processamento funciona com PALAVRAS, números 123 e pontuações diferentes! Estou testando o stemming e a lematização. Site: http://teste.com.br. Curti muito o app! #top @usuario\n",
            "\n",
            "Tokens (NLTK): ['olá', 'mundo', 'teste', 'completo', 'ver', 'préprocessamento', 'funciona', 'palavras', 'números', 'pontuações', 'diferentes', 'testando', 'stemming', 'lematização', 'site', 'curti', 'app']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Processamento com NLTK (com stemming) ---\n",
        "print(\"--- Processamento com NLTK (com stemming RSLP) ---\")\n",
        "tokens_nltk_steeming = preprocess_text(texto_completo_exemplo, apply_stemming=True)\n",
        "print(f\"Tokens (NLTK + Stemming): {tokens_nltk_steeming}\")"
      ],
      "metadata": {
        "id": "LeheBqICS7EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888eb3cd-bd91-407a-c859-3076ac672e3f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processamento com NLTK (com stemming RSLP) ---\n",
            "Tokens (NLTK + Stemming): ['olá', 'mund', 'test', 'complet', 'ver', 'préprocess', 'func', 'palavr', 'númer', 'pontu', 'difer', 'test', 'stemming', 'lemat', 'sit', 'curt', 'app']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Exercícios Práticos: Raspando e Pré-processando Comentários de Apps\n",
        "\n",
        "Agora é a sua vez de colocar a mão na massa! Vamos aplicar o que aprendemos raspando comentários de um aplicativo da Google Play Store e pré-processando-os.\n",
        "\n",
        "Usaremos a biblioteca `google-play-scraper`. Você precisará instalá-la."
      ],
      "metadata": {
        "id": "Djr2w4Wwih07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-play-scraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCm6qYJyimhj",
        "outputId": "04732a4f-fef3-49d2-daa5-f35a18947852"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-play-scraper in /usr/local/lib/python3.11/dist-packages (1.2.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercício 1: Raspar Comentários de um App**\n",
        "\n",
        "Escolha um aplicativo na Google Play Store. Você pode encontrar o ID do aplicativo na URL da página dele. Por exemplo, a URL do WhatsApp é `https://play.google.com/store/apps/details?id=com.whatsapp`. O ID é `com.whatsapp`.\n",
        "\n",
        "Vamos raspar uma quantidade limitada de comentários para este exercício."
      ],
      "metadata": {
        "id": "GbOOyuHbiraf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google_play_scraper import Sort, reviews\n",
        "import pandas as pd\n",
        "\n",
        "# Substitua 'com.whatsapp' pelo ID do aplicativo que você quer raspar\n",
        "APP_ID = 'com.google.android.youtube' # Exemplo: WhatsApp\n",
        "NUM_REVIEWS = 20 # Quantidade de reviews a raspar (máx 200 por chamada com Sort.MOST_RELEVANT)\n",
        "\n",
        "all_reviews = []"
      ],
      "metadata": {
        "id": "zE7iT-Fziqq-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A API scraper geralmente só retorna 200 por vez para a ordenação mais relevante\n",
        "# Se quisesse mais, precisaria usar outra ordenação ou múltiplos tokens,\n",
        "# mas para o exercício, 200 é suficiente.\n",
        "try:\n",
        "    result, continuation_token = reviews(\n",
        "        APP_ID,\n",
        "        lang='pt', # Idioma dos reviews (português)\n",
        "        country='br', # País\n",
        "        sort=Sort.MOST_RELEVANT, # Tipo de ordenação\n",
        "        count=NUM_REVIEWS # Quantidade\n",
        "    )\n",
        "    all_reviews.extend(result)\n",
        "\n",
        "    print(f\"Raspagem concluída. Foram encontrados {len(all_reviews)} reviews.\")\n",
        "\n",
        "    if len(all_reviews) > 0:\n",
        "        # Exibir os primeiros 5 reviews raspados\n",
        "        print(\"\\nPrimeiros 5 reviews:\")\n",
        "        for i, review in enumerate(all_reviews[:5]):\n",
        "            print(f\"--- Review {i+1} ---\")\n",
        "            print(f\"Usuário: {review.get('userName', 'N/A')}\")\n",
        "            print(f\"Pontuação: {review.get('score', 'N/A')}\")\n",
        "            print(f\"Data: {review.get('at', 'N/A')}\")\n",
        "            print(f\"Conteúdo: {review.get('content', 'N/A')}\")\n",
        "            print(\"-----------------\")\n",
        "\n",
        "        # Criar um DataFrame pandas com os reviews (útil para inspecionar, mas não essencial para o pré-processamento puro)\n",
        "        reviews_df = pd.DataFrame(all_reviews)\n",
        "        print(f\"\\nDataFrame criado com {len(reviews_df)} reviews.\")\n",
        "        # Extrair apenas o conteúdo dos reviews para pré-processamento\n",
        "        review_contents = reviews_df['content'].dropna().tolist() # Remover reviews sem conteúdo e converter para lista\n",
        "\n",
        "        print(f\"\\nTotal de conteúdos de reviews extraídos para processamento: {len(review_contents)}\")\n",
        "    else:\n",
        "        print(\"Nenhum review encontrado para o APP_ID especificado.\")\n",
        "        reviews_df = pd.DataFrame()\n",
        "        review_contents = [] # Lista vazia se não encontrar reviews\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante a raspagem dos reviews: {e}\")\n",
        "    reviews_df = pd.DataFrame()\n",
        "    review_contents = [] # Garante que review_contents é uma lista mesmo em caso de erro"
      ],
      "metadata": {
        "id": "UKAhbRask6L1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deef02dc-5210-4c06-d538-ad5a1a663358"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raspagem concluída. Foram encontrados 20 reviews.\n",
            "\n",
            "Primeiros 5 reviews:\n",
            "--- Review 1 ---\n",
            "Usuário: Jebs freire\n",
            "Pontuação: 2\n",
            "Data: 2025-05-27 14:30:32\n",
            "Conteúdo: Tem algum Bug nos comentários, vc precisa enviar várias vezes, e é apagado. E não é nada proibido, até uma carinha de emoji feliz não fica, é apagado. O que é isso? Os anúncios são chatos, tem alguns que são longos. Quando é curto ainda dá pra aguentar, mas se vc não pula vem outro anúncio e outro e outro... E demorados. Além disso, está com um bug estranho quando avanço 10 seg, se vc tenta avançar mais, trava, parece um disco arranhado ou um dj. Arrumem isso!\n",
            "-----------------\n",
            "--- Review 2 ---\n",
            "Usuário: Pikachu de Bigode\n",
            "Pontuação: 4\n",
            "Data: 2025-05-29 20:21:25\n",
            "Conteúdo: Bom, aqui estou eu dnv, o problema do pop-up desde aquele tempo tem sido resolvido pelo q dá para ver, mas agr é uma sugestão, seria legal na aba de histórico acrescentar categorias, por exemplo: assisto um clipe de música, então ele fica na categoria de música; assisto um vídeo sobre jogos ou algum jogo, aí ele fica na categoria de jogos, tipo isso. Eu tento achar vídeos de notícias ou vídeo clipes e o meu histórico é muito extenso, por isso seria bom essa organização\n",
            "-----------------\n",
            "--- Review 3 ---\n",
            "Usuário: Irya\n",
            "Pontuação: 3\n",
            "Data: 2025-05-09 11:55:46\n",
            "Conteúdo: Sempre gostei do YouTube, mas a publicidade está excessiva. Agora, até ao pausar um vídeo, sou forçado a ver anúncios. Isso compromete a usabilidade de um aplicativo que deveria ser prático. Com o tempo, a plataforma priorizou o lucro em vez da experiência do usuário. Antes, a tecnologia era desenvolvida para ser útil; hoje, tudo gira em torno de monetização agressiva. O YouTube continua sendo uma ferramenta valiosa, mas essa abordagem excessiva prejudica sua qualidade. Muitos anúncios falsos.\n",
            "-----------------\n",
            "--- Review 4 ---\n",
            "Usuário: Um usuário do Google\n",
            "Pontuação: 4\n",
            "Data: 2025-04-27 06:54:44\n",
            "Conteúdo: Amo usar o YouTube, uso completamente todos os dias então é bastante mesmo mais... essas últimas atualizações deixaram o app um pouco complicado, por exemplo... o histórico, que as vezes eu quero apagar o histórico de vários vídeos do shots que eu assisti, mais a cada vídeo removido eu tenho que sair do histórico e entrar novamente ou recarregar, anteriormente era só ir removendo um por um, que o próximo vinha em seguida prático e rápido. Por favor voltem como estava antes, era muito melhor.\n",
            "-----------------\n",
            "--- Review 5 ---\n",
            "Usuário: Pâmela\n",
            "Pontuação: 4\n",
            "Data: 2025-01-29 18:56:11\n",
            "Conteúdo: Irei resumir: Vídeos, lives, shorts, comentários, postagens, anúncios, músicas, mvs, tem como postar muitas coisas diferentes, amei a atualização de definir um horário para postar, prático. Tenho algumas coisas que não gostei: Os anúncios agora aparecem embaixo do vídeo, eu sempre que vou ler o título, entra num site. Também não gosto de que agora não tem aquela opção de escrever no \"subtítulo\" do vídeo. Acaba o limite de letras muito rápido e não dá para colocar todas as tags que gostaria.\n",
            "-----------------\n",
            "\n",
            "DataFrame criado com 20 reviews.\n",
            "\n",
            "Total de conteúdos de reviews extraídos para processamento: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercício 2: Pré-processar os Comentários Raspados**\n",
        "\n",
        "Agora, aplique a função `preprocess_text` que criamos para limpar os conteúdos dos reviews raspados. Você pode experimentar diferentes configurações (remover ou não números, pontuação, stop words, usar stemming)."
      ],
      "metadata": {
        "id": "ojD6QQDblUn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilize a função preprocess_text definida anteriormente\n",
        "\n",
        "processed_reviews = []\n",
        "\n",
        "if review_contents:\n",
        "    print(\"Pré-processando os reviews com NLTK (Stemming)...\")\n",
        "\n",
        "    # Escolha suas opções de pré-processamento\n",
        "    preprocess_options = {\n",
        "        'remove_numbers': True,\n",
        "        'remove_punctuation': True,\n",
        "        'remove_stopwords': True,\n",
        "        'apply_stemming': True # Experimente True e False\n",
        "    }\n",
        "\n",
        "    for i, review_text in enumerate(review_contents):\n",
        "        if (i + 1) % 50 == 0: # Apenas para mostrar o progresso a cada 50 reviews\n",
        "            print(f\"  Processando review {i+1}/{len(review_contents)}...\")\n",
        "\n",
        "        # Aplica a função de pré-processamento\n",
        "        if review_text and isinstance(review_text, str):\n",
        "            processed_tokens = preprocess_text(review_text, **preprocess_options)\n",
        "            processed_reviews.append(processed_tokens)\n",
        "        else:\n",
        "            # Adiciona uma lista vazia ou trata como preferir para reviews sem conteúdo\n",
        "            processed_reviews.append([])\n",
        "\n",
        "\n",
        "    print(\"Pré-processamento concluído!\")\n",
        "\n",
        "    # Exibir os tokens pré-processados dos primeiros 5 reviews (apenas se houver reviews processados)\n",
        "    if processed_reviews:\n",
        "        print(\"\\nPrimeiros 5 reviews pré-processados (NLTK Stemming):\")\n",
        "        for i, tokens in enumerate(processed_reviews[:5]):\n",
        "            print(f\"--- Review {i+1} (Tokens) ---\")\n",
        "            print(tokens)\n",
        "            print(\"----------------------------\")\n",
        "    else:\n",
        "         print(\"\\nNenhum review processado.\")\n",
        "\n",
        "else:\n",
        "    print(\"Nenhum conteúdo de review para processar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPCH_SK-lgTh",
        "outputId": "858c9cc8-6d45-4733-c082-6732a38d360a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pré-processando os reviews com NLTK (Stemming)...\n",
            "Pré-processamento concluído!\n",
            "\n",
            "Primeiros 5 reviews pré-processados (NLTK Stemming):\n",
            "--- Review 1 (Tokens) ---\n",
            "['algum', 'bug', 'coment', 'vc', 'precis', 'envi', 'vár', 'vez', 'apag', 'nad', 'proib', 'car', 'emoj', 'feliz', 'fic', 'apag', 'anúnci', 'chat', 'algum', 'long', 'curt', 'aind', 'dá', 'pra', 'aguent', 'vc', 'pul', 'vem', 'outr', 'anúnci', 'outr', 'outr', 'demor', 'além', 'diss', 'bug', 'estranh', 'avanç', 'seg', 'vc', 'tent', 'avanç', 'tr', 'parec', 'disc', 'arranh', 'dj', 'arrum']\n",
            "----------------------------\n",
            "--- Review 2 (Tokens) ---\n",
            "['bom', 'aqu', 'dnv', 'problem', 'popup', 'desd', 'temp', 'sid', 'resolv', 'dá', 'ver', 'agr', 'sugest', 'legal', 'aba', 'histór', 'acrescent', 'categor', 'exempl', 'assist', 'clip', 'músic', 'ent', 'fic', 'categor', 'músic', 'assist', 'víde', 'sobr', 'jog', 'algum', 'jog', 'aí', 'fic', 'categor', 'jog', 'tip', 'tent', 'ach', 'víde', 'notíc', 'víde', 'clip', 'histór', 'extens', 'bom', 'organiz']\n",
            "----------------------------\n",
            "--- Review 3 (Tokens) ---\n",
            "['sempr', 'gost', 'youtub', 'public', 'excess', 'agor', 'paus', 'víde', 'forç', 'ver', 'anúnci', 'compromet', 'usabil', 'aplic', 'dev', 'prát', 'temp', 'plataform', 'prioriz', 'lucr', 'vez', 'experi', 'usu', 'ant', 'tecnolog', 'desenvolv', 'útil', 'hoj', 'tud', 'gir', 'torn', 'monet', 'agress', 'youtub', 'continu', 'send', 'ferrament', 'vali', 'abord', 'excess', 'prejud', 'qual', 'muit', 'anúnci', 'fals']\n",
            "----------------------------\n",
            "--- Review 4 (Tokens) ---\n",
            "['amo', 'us', 'youtub', 'uso', 'complet', 'tod', 'dia', 'ent', 'bast', 'últ', 'atual', 'deix', 'app', 'pouc', 'complic', 'exempl', 'histór', 'vez', 'quer', 'apag', 'histór', 'vári', 'víde', 'shot', 'assist', 'cad', 'víde', 'remov', 'sair', 'histór', 'entr', 'nov', 'recarreg', 'anteri', 'ir', 'remov', 'próx', 'vinh', 'segu', 'prát', 'rápid', 'favor', 'volt', 'ant', 'melhor']\n",
            "----------------------------\n",
            "--- Review 5 (Tokens) ---\n",
            "['ire', 'resum', 'víde', 'liv', 'short', 'coment', 'post', 'anúnci', 'músic', 'mv', 'post', 'muit', 'cois', 'difer', 'ame', 'atual', 'defin', 'hor', 'post', 'prát', 'algum', 'cois', 'gost', 'anúnci', 'agor', 'aparec', 'embaix', 'víde', 'sempr', 'vou', 'ler', 'títul', 'entr', 'sit', 'gost', 'agor', 'opç', 'escrev', 'subtítul', 'víde', 'acab', 'limit', 'letr', 'rápid', 'dá', 'coloc', 'tod', 'tag', 'gost']\n",
            "----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercício 3: Análise Básica - Contagem de Frequência de Palavras**\n",
        "\n",
        "Com os tokens pré-processados, uma análise básica interessante é contar a frequência de cada palavra. Quais são as palavras mais comuns nos comentários após a limpeza? Isso pode dar insights sobre os tópicos mais mencionados nos reviews."
      ],
      "metadata": {
        "id": "COCzCvXMlppH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter # Já importado no início\n",
        "import itertools # Já importado no início\n",
        "\n",
        "if processed_reviews:\n",
        "    print(\"\\n--- Análise de Frequência (NLTK Stemming) ---\")\n",
        "    # Achatar a lista de listas de tokens em uma única lista\n",
        "    all_tokens_nltk_stem = list(itertools.chain.from_iterable(filter(None, processed_reviews)))\n",
        "    print(f\"Total de tokens (NLTK Stemming): {len(all_tokens_nltk_stem)}\")\n",
        "\n",
        "    if all_tokens_nltk_stem:\n",
        "        token_counts_nltk_stem = Counter(all_tokens_nltk_stem)\n",
        "\n",
        "        # Exibir os 30 tokens mais comuns\n",
        "        print(\"\\n30 tokens mais comuns (NLTK Stemming):\")\n",
        "        for token, count in token_counts_nltk_stem.most_common(30):\n",
        "            print(f\"'{token}': {count}\")\n",
        "    else:\n",
        "         print(\"Nenhum token para contar (NLTK Stemming).\")\n",
        "else:\n",
        "    print(\"\\nNenhum review processado para contar a frequência.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP0qkkZklzzV",
        "outputId": "eae4099f-7dd3-4dff-9d40-4bdc15b73478"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Análise de Frequência (NLTK Stemming) ---\n",
            "Total de tokens (NLTK Stemming): 891\n",
            "\n",
            "30 tokens mais comuns (NLTK Stemming):\n",
            "'víde': 45\n",
            "'anúnci': 18\n",
            "'bug': 15\n",
            "'algum': 13\n",
            "'app': 13\n",
            "'tel': 13\n",
            "'pra': 12\n",
            "'fic': 11\n",
            "'youtub': 10\n",
            "'dá': 9\n",
            "'outr': 9\n",
            "'sempr': 8\n",
            "'atual': 8\n",
            "'ver': 7\n",
            "'assist': 7\n",
            "'paus': 7\n",
            "'aplic': 7\n",
            "'tod': 7\n",
            "'nov': 7\n",
            "'vez': 6\n",
            "'pul': 6\n",
            "'temp': 6\n",
            "'músic': 6\n",
            "'gost': 6\n",
            "'us': 6\n",
            "'aparec': 6\n",
            "'coloc': 6\n",
            "'simples': 6\n",
            "'chei': 6\n",
            "'histór': 5\n"
          ]
        }
      ]
    }
  ]
}