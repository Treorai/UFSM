{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPajGktQSMqbgyG8qBhsZJD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introdução ao Processamento de Linguagem Natural (PLN) e Pré-processamento de Texto\n","\n","Este notebook tem como objetivo introduzir os conceitos fundamentais de Processamento de Linguagem Natural (PLN) e, em particular, as técnicas essenciais de pré-processamento de texto. O pré-processamento é uma etapa crucial em praticamente qualquer tarefa de PLN, pois transforma dados textuais brutos em um formato mais limpo e estruturado, adequado para análise e modelagem computacional.\n","\n","Vamos cobrir:\n","1.  O que é PLN e por que o pré-processamento é importante.\n","2.  Principais etapas do pré-processamento de texto.\n","3.  Expressões Regulares (Regex)\n","4.  Stemming (Radicalização) vs. Lematização.\n","5.  Exemplos práticos de código utilizando a biblioteca NLTK e o módulo `re` em Python.\n","6.  Exercícios práticos para aplicar os conhecimentos.\n","\n","---\n","\n","\n"],"metadata":{"id":"8Nr8jxxCcECX"}},{"cell_type":"markdown","source":["# 1. O que é PLN e Por que Pré-processar Texto?\n","\n","### O que é PLN?\n","\n","**Processamento de Linguagem Natural (PLN)**, do inglês *Natural Language Processing (NLP)*, é um campo da inteligência artificial que se concentra na interação entre computadores e linguagens humanas (naturais). O objetivo é permitir que os computadores \"entendam\", \"interpretem\" e \"manipulem\" a linguagem humana.\n","\n","Aplicações de PLN incluem:\n","*   Assistentes virtuais (Siri, Google Assistant, Alexa)\n","*   Tradutores automáticos (Google Translate)\n","*   Análise de Sentimento (entender a opinião em textos)\n","*   Chatbots\n","*   Sumarização automática de textos\n","*   Sistemas de busca (Google Search)\n","\n","### Por que Pré-processar Texto?\n","\n","A linguagem humana é rica, complexa e muitas vezes ambígua. Textos brutos contêm ruídos, variações gramaticais, erros de digitação, pontuação e formatação que podem dificultar o processamento computacional.\n","\n","O **pré-processamento de texto** é o processo de limpeza e preparação dos dados textuais para que possam ser utilizados de forma eficaz em tarefas de PLN. É como limpar e organizar ingredientes antes de cozinhar; os resultados serão muito melhores!\n","\n","**Objetivos do pré-processamento:**\n","*   Reduzir ruído e redundância.\n","*   Padronizar o texto.\n","*   Converter texto em um formato mais fácil de analisar (e.g., listas de palavras).\n","*   Reduzir a dimensionalidade dos dados (número de palavras únicas).\n","\n","---"],"metadata":{"id":"iiYpWHH3cm0V"}},{"cell_type":"markdown","source":["# 2. Principais Etapas do Pré-processamento de Texto\n","\n","Vamos explorar algumas das etapas mais comuns e importantes no pré-processamento de texto. Usaremos a biblioteca NLTK (Natural Language Toolkit) em Python, que é uma das ferramentas mais populares para PLN, e o módulo `re` para Expressões Regulares.\n","\n","Primeiro, precisamos instalar a biblioteca NLTK e baixar os recursos necessários."],"metadata":{"id":"Mx7bWzFOc4Dn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqmnERJOLSrZ"},"outputs":[],"source":["import nltk\n","import re\n","import string\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import RSLPStemmer\n","from collections import Counter\n","import itertools\n","\n","# Baixar recursos necessários do NLTK\n","# punkt: para tokenização de sentenças e palavras\n","# stopwords: lista de palavras comuns a serem removidas\n","# rslp: stemmer para português\n","# wordnet: dicionário para lematização (principalmente inglês)\n","# averaged_perceptron_tagger: para POS tagging (usado na lematização)\n","\n","print(\"Verificando e baixando recursos NLTK...\")\n","\n","nltk_resources = ['punkt', 'stopwords', 'rslp', 'wordnet', 'averaged_perceptron_tagger']\n","for resource in nltk_resources:\n","    try:\n","        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}' if resource in ['stopwords', 'wordnet'] else f'stemmers/{resource}' if resource == 'rslp' else f'taggers/{resource}')\n","        print(f\"Recurso '{resource}' já baixado.\")\n","    except LookupError:\n","         print(f\"Recurso '{resource}' não encontrado localmente. Baixando...\")\n","         nltk.download(resource)\n","\n","\n","print(\"Verificação e download de recursos NLTK concluídos!\")\n","\n","# Carregar stop words do NLTK e stemmer (para uso posterior)\n","stop_words_portugues = set(stopwords.words('portuguese'))\n","stemmer_portugues = RSLPStemmer()"]},{"cell_type":"markdown","source":["## a) Converter para Minúsculas (Lowercasing)\n","\n","Converter todo o texto para minúsculas ajuda a garantir que palavras que diferem apenas na capitalização sejam tratadas como a mesma palavra (ex: \"Palavra\", \"palavra\", \"PALAVRA\")."],"metadata":{"id":"NEY6uUgXdR-U"}},{"cell_type":"markdown","source":["##b) Remover Pontuação\n","Pontuações (vírgulas, pontos, pontos de exclamação, etc.) geralmente não carregam significado semântico para a maioria das tarefas de PLN e podem ser removidas."],"metadata":{"id":"V60UY_RHdgNl"}},{"cell_type":"markdown","source":["## c) Remover Números\n","Dependendo da tarefa, números podem ser irrelevantes. Já mostramos um exemplo usando o módulo re (Expressões Regulares) para isso. Veremos mais sobre Regex em detalhes a seguir."],"metadata":{"id":"zSkx2UQKd0RX"}},{"cell_type":"markdown","source":["A remoção de números (`re.sub(r'\\d+', '', text)`) é uma etapa comum em muitos pipelines de pré-processamento, mas a decisão de fazê-lo **depende totalmente da sua tarefa de PLN**.\n","\n","**Razões Comuns para Remover Números:**\n","\n","1.  **Reduzir Ruído e Dimensionalidade:** Para tarefas como análise de sentimento geral ou modelagem de tópicos, o valor numérico exato (como \"100\", \"2023\", \"5\") muitas vezes não contribui para o *significado principal* da frase. Um review que diz \"O produto custa 100 reais\" e outro que diz \"O produto custa 200 reais\" podem ter o mesmo sentimento (\"neutro\" ou \"positivo/negativo\" dependendo do contexto da frase). Tratar \"100\" e \"200\" como tokens separados pode inchar seu vocabulário sem adicionar valor semântico relevante para *essa tarefa específica*.\n","2.  **Focar no Conteúdo Textual:** Muitos algoritmos de PLN focam na relação e frequência das palavras. Números podem quebrar padrões de texto (\"comprou em [NÚMERO] e gostou\") sem que o número em si seja o foco da análise.\n","3.  **Padronização:** Em alguns casos, você pode querer saber *que* um número estava presente, mas não qual era. Substituir todos os números por um token especial (como `<NUM>`) padroniza essa informação e ainda reduz a dimensionalidade (em vez de infinitos números possíveis, você tem apenas um token `<NUM>`).\n","\n","**Quando NÃO Remover Números (ou Tratá-los Diferente):**\n","\n","Você **NÃO DEVE** remover números se a informação numérica for crucial para a tarefa de PLN. Exemplos:\n","\n","*   **Análise de Reviews de Produtos/Apps:** Se a tarefa for analisar *por que* um produto recebeu 1 estrela vs. 5 estrelas, o número na frase \"Recebi 1 estrela mas o produto é ótimo\" ou \"Dou 5 estrelas\" é *extremamente* importante.\n","*   **Análise Financeira:** Valores monetários (\"lucro de 1 milhão\"), percentuais, datas, etc., são o cerne da análise.\n","*   **Extração de Informação:** Se você quer extrair endereços, números de telefone, datas de validade, versões de software (\"versão 3.0\"), remover números inviabilizaria a tarefa.\n","*   **Análise de Textos Científicos/Técnicos:** Medições, fórmulas, códigos de produtos, etc., são essenciais."],"metadata":{"id":"iJbA25GVmxSD"}},{"cell_type":"markdown","source":["## d) Remover Espaços em Branco Extras\n","Múltiplos espaços entre palavras ou espaços no início/fim da string podem ser limpos. Uma forma simples é usar ``split()`` e ``join()``. Outra forma muito eficiente é usar Expressões Regulares, como veremos adiante."],"metadata":{"id":"j6UPlGJSeCQc"}},{"cell_type":"markdown","source":["---\n","\n","# 3. Expressões Regulares (Regex) como aliadas no pré-processamento\n","\n","**Expressões Regulares (Regex ou Regexp)** são sequências de caracteres que definem um *padrão de busca*. São incrivelmente poderosas para encontrar, substituir ou extrair padrões complexos em texto de forma concisa. Embora a sintaxe possa parecer intimidadora no início, dominar o básico de Regex é um superpoder no processamento de texto.\n","\n","Em Python, o módulo `re` oferece suporte nativo para Expressões Regulares.\n","\n","### Por que usar Regex em PLN?\n","\n","*   **Limpeza de Ruído:** Remover padrões indesejados como tags HTML, URLs, emails, caracteres especiais, múltiplos espaços.\n","*   **Extração de Informação:** Encontrar números de telefone, datas, menções de usuários, hashtags, etc.\n","*   **Validação de Formato:** Verificar se uma string segue um formato específico (ex: formato de email, CPF).\n","*   **Substituição:** Substituir padrões encontrados por outro texto (como fizemos para remover números).\n","\n","### Sintaxe Básica de Regex\n","\n","Aqui estão alguns dos elementos de padrão mais comuns:\n","\n","*   `a`, `1`, `@` - Caracteres Literais: correspondem exatamente a eles mesmos.\n","*   `.` (Ponto): Corresponde a qualquer caractere (exceto quebra de linha `\\n`).\n","*   `*` (Asterisco): Corresponde a **zero ou mais** ocorrências do elemento anterior. Ex: `a*` corresponde a \"\", \"a\", \"aa\", \"aaa\"...\n","*   `+` (Mais): Corresponde a **uma ou mais** ocorrências do elemento anterior. Ex: `a+` corresponde a \"a\", \"aa\", \"aaa\"... mas não \"\".\n","*   `?` (Interrogação): Corresponde a **zero ou uma** ocorrência do elemento anterior. Ex: `a?` corresponde a \"\" ou \"a\".\n","*   `|` (Barra Vertical): Operador **OR**. Ex: `gato|cachorro` corresponde a \"gato\" ou \"cachorro\".\n","*   `()` (Parênteses): Agrupamento e Captura. Permite aplicar operadores (`*`, `+`, `?`) a um grupo inteiro e capturar partes da string.\n","*   `[]` (Colchetes): Conjunto de Caracteres. Corresponde a **qualquer um** dos caracteres dentro dos colchetes. Ex: `[aeiou]` corresponde a qualquer vogal minúscula. `[0-9]` corresponde a qualquer dígito. `[a-zA-Z]` corresponde a qualquer letra (maiúscula ou minúscula).\n","*   `-` (Hífen) dentro de `[]`: Define um Intervalo. Ex: `[a-z]` (letras minúsculas de a a z), `[0-9]` (dígitos de 0 a 9).\n","*   `^` (Circunflexo) dentro de `[]`: Negação. Corresponde a **qualquer caractere que NÃO ESTÁ** no conjunto. Ex: `[^0-9]` corresponde a qualquer caractere que não seja um dígito.\n","*   `\\` (Barra Invertida): Caractere de Escape. Usado para \"escapar\" caracteres especiais (como `.`, `*`, `+`, `?`, `|`, `(`, `)`, `[`, `]`, `{`, `}`, `^`, `$`, `\\` ) ou para introduzir sequências especiais.\n","    *   `\\d`: Corresponde a um dígito (`[0-9]`).\n","    *   `\\D`: Corresponde a um não-dígito (`[^0-9]`).\n","    *   `\\w`: Corresponde a um caractere de \"palavra\" (`[a-zA-Z0-9_]`).\n","    *   `\\W`: Corresponde a um não-caractere de \"palavra\" (`[^a-zA-Z0-9_]`).\n","    *   `\\s`: Corresponde a um caractere de espaço em branco (espaço, tab `\\t`, quebra de linha `\\n`, retorno de carro `\\r`, feed de formulário `\\f`, tab vertical `\\v`).\n","    *   `\\S`: Corresponde a um não-caractere de espaço em branco.\n","*   `^` (Circunflexo) fora de `[]`: Corresponde ao **início** da string.\n","*   `$` (Cifrão): Corresponde ao **fim** da string.\n","\n","\n","\n","**Strings Raw (Brutas) em Python (`r\"...\"`)**: É uma boa prática usar strings raw para padrões Regex em Python (prefixo `r`). Isso impede que as barras invertidas (`\\`) sejam interpretadas pelo Python como sequências de escape de string Python (ex: `\\n` para nova linha) antes de serem passadas para o motor de Regex.\n","\n","### Funções do Módulo `re` (Principais para Pré-processamento)\n","\n","*   `re.search(pattern, string)`: Procura pela *primeira* ocorrência do padrão na string. Retorna um objeto `Match` se encontrar, `None` caso contrário.\n","*   `re.findall(pattern, string)`: Encontra *todas* as ocorrências não sobrepostas do padrão na string e retorna uma **lista** de strings correspondentes.\n","*   `re.sub(pattern, repl, string)`: Substitui todas as ocorrências do `pattern` encontrado na `string` pela string `repl`. É a função mais usada para limpeza.\n","\n","Vamos ver alguns exemplos práticos:"],"metadata":{"id":"gpz8BX_2ecvQ"}},{"cell_type":"code","source":["texto_regex = \"Meu email é teste@example.com e meu site é https://www.site.org. Meu telefone é (11) 98765-4321. Há também números 12345 e símbolos !!!\"\n","\n","print(f\"Texto original: {texto_regex}\\n\")\n","\n","# Exemplo 1: Encontrar todos os números\n"],"metadata":{"id":"Fk0Dkx3IfX-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exemplo 2: Substituir todos os espaços em branco múltiplos por um único espaço\n","# \\s+ significa \"um ou mais caracteres de espaço em branco\"\n"],"metadata":{"id":"Uz-X0rr9f7CM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exemplo 3: Remover caracteres que não são letras, números ou espaços\n","# [^\\w\\s] significa \"qualquer caractere que NÃO SEJA (\\^) um caractere de palavra (\\w) OU (\\s) um espaço em branco\"\n","# Poderíamos substituir por um espaço ou uma string vazia\n","texto_com_especiais = \"Texto! Com? Simbolos@ E. Pontuacao,\"\n"],"metadata":{"id":"uxU_MgJ_f7bi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exemplo 4: Remover URLs (exemplo comum em reviews online)\n","texto_com_url = \"Produto bom! Veja em: http://loja.com/produto1 ou https://outraloja.org\"\n"],"metadata":{"id":"b1yeZx9Df8pl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como visto, Regex é uma ferramenta flexível que pode complementar ou substituir algumas das etapas de limpeza manuais que vimos anteriormente. No nosso pipeline de pré-processamento (seção 2), já usamos `re.sub` para remover números. Poderíamos usá-la também para remover pontuação ou limpar espaços."],"metadata":{"id":"LqeGGrARgLU7"}},{"cell_type":"markdown","source":["# 4. Continuação das Etapas de Pré-processamento\n","\n","Agora que temos uma ferramenta poderosa para limpeza de padrões, vamos continuar com as etapas mais focadas na estrutura e significado das palavras.\n","\n","### e) Tokenização\n","\n","Tokenização é o processo de dividir uma string de texto em pedaços menores chamados **tokens**. Os tokens podem ser palavras, sub-palavras, ou até mesmo frases, dependendo da granularidade desejada. Geralmente, tokenizamos em palavras."],"metadata":{"id":"qoR0A3aHgnKz"}},{"cell_type":"code","source":["nltk.download('punkt_tab')"],"metadata":{"id":"P4xoN8BBPVgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texto_para_tokenizar = \"Tokenização é o processo de dividir texto em palavras.\"\n"],"metadata":{"id":"l0Rq9XXmgL-s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### f) Remover Stop Words\n","\n","**Stop words** são palavras muito comuns em uma língua que geralmente não adicionam muito significado a uma frase (ex: \"de\", \"a\", \"o\", \"e\", \"é\", \"em\", \"para\", etc.). Removê-las pode reduzir o tamanho do vocabulário e melhorar o desempenho em algumas tarefas.\n","\n","O NLTK possui listas de stop words para vários idiomas."],"metadata":{"id":"zTteGuExgzwN"}},{"cell_type":"code","source":["texto_com_stopwords = \"Esta é uma frase de exemplo com muitas palavras comuns e irrelevantes.\"\n","tokens = word_tokenize(texto_com_stopwords, language='portuguese')\n","\n","# Convertendo tokens para minúsculas ANTES de verificar se são stop words\n","tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stop_words_portugues]\n","\n","print(f\"Tokens originais: {tokens}\")\n","print(f\"Stop words em português (exemplo): {list(stop_words_portugues)[:10]}...\") # Mostrar apenas as primeiras 10\n","print(f\"Tokens sem stop words: {tokens_sem_stopwords}\")"],"metadata":{"id":"SVwkF4Rig1aV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Stemming (Radicalização) vs. Lematização\n","\n","Essas duas técnicas têm um objetivo semelhante: reduzir palavras flexionadas ou derivadas para uma forma base. No entanto, a abordagem e o resultado são diferentes.\n","\n","### Stemming (Radicalização)\n","\n","**Stemming** é um processo heurístico que corta sufixos (e às vezes prefixos) das palavras para chegar a um \"radical\" (stem). O radical resultante **não necessariamente é uma palavra real** da língua. É um processo mais rápido e menos preciso, útil para agrupar palavras com significados semelhantes.\n","\n","Exemplo em inglês: \"running\", \"runs\", \"ran\" podem ser reduzidos para o radical \"run\". \"beautiful\", \"beauty\" podem ser reduzidos para \"beauti\".\n","\n","Para português, o NLTK oferece o RSLP Stemmer (Remoção de Sufixos da Língua Portuguesa)."],"metadata":{"id":"ExwuMA-XhAxF"}},{"cell_type":"code","source":["# stemmer_portugues já carregado no início\n","\n","palavras = [\"correndo\", \"correram\", \"corre\", \"bonito\", \"beleza\", \"mulheres\", \"mulher\", \"linguagem\", \"linguagens\"]\n","\n","\n","# Note que alguns radicais (\"corr\", \"mulher\") são palavras reais,\n","# mas outros (\"bonit\", \"linguag\") não são necessariamente."],"metadata":{"id":"p8JkSF8phFSP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Lematização** é um processo mais sofisticado que utiliza vocabulário (um dicionário de palavras e suas formas) e análise morfológica para reduzir palavras flexionadas à sua forma base ou dicionário, conhecida como **lema**. O lema resultante **sempre é uma palavra real** da língua. É um processo mais lento, mas geralmente mais preciso que o stemming.\n","\n","Exemplo em inglês: \"running\", \"runs\", \"ran\" são lematizados para o lema \"run\". \"better\" é lematizado para \"good\". \"am\", \"is\", \"are\" são lematizados para \"be\".\n","\n","O NLTK possui o `WordNetLemmatizer`, mas ele funciona melhor para inglês e requer a especificação da Parte da Fala (POS tag) da palavra para um resultado mais preciso. Lematização robusta para português geralmente requer bibliotecas mais avançadas como spaCy ou outras ferramentas específicas para português, pois o NLTK WordNet não cobre bem o vocabulário português.\n","\n","Vamos demonstrar o conceito com o lemmatizer do NLTK para inglês e discutir o caso do português."],"metadata":{"id":"kpRYreybhTbJ"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","# Para lematizar corretamente, muitas vezes precisamos do POS tag (Parte da Fala)\n","from nltk.corpus import wordnet\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","# Função auxiliar para obter o POS tag no formato que o WordNetLemmatizer entende\n","def get_wordnet_pos(tag):\n","    \"\"\"Converte o POS tag do NLTK no formato do WordNet\"\"\"\n","    if tag.startswith('J'): # Adjetivo\n","        return wordnet.ADJ\n","    elif tag.startswith('V'): # Verbo\n","        return wordnet.VERB\n","    elif tag.startswith('N'): # Substantivo\n","        return wordnet.NOUN\n","    elif tag.startswith('R'): # Advérbio\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN # Padrão para substantivo"],"metadata":{"id":"Ab-m87lehdeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger_eng')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mUh3FOkSQym","executionInfo":{"status":"ok","timestamp":1749750978917,"user_tz":180,"elapsed":159,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"7b7aa57f-8f7c-47f5-b4c5-224ee0dc0d27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Exemplo em Inglês (para ilustrar o conceito com WordNetLemmatizer)\n","palavras_en = [\"running\", \"runs\", \"ran\", \"better\", \"geese\", \"children\", \"is\", \"are\"]\n","\n","# Obter POS tags para cada palavra (simplificado para demonstração)\n","# Em um pipeline real, processaríamos a frase inteira com um POS tagger\n","pos_tags_en = nltk.pos_tag(palavras_en)\n","\n","lemas_en = [\n","    lemmatizer.lemmatize(palavra, pos=get_wordnet_pos(tag))\n","    for palavra, tag in pos_tags_en\n","]\n","\n","print(f\"Palavras originais (Inglês): {palavras_en}\")\n","print(f\"POS Tags (Inglês - simplificado): {pos_tags_en}\")\n","print(f\"Lemas (Inglês): {lemas_en}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YGQINFUjhesk","executionInfo":{"status":"ok","timestamp":1749750985098,"user_tz":180,"elapsed":3469,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"7cd5bdba-55ac-42b5-81f3-7cf1e9952b44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Palavras originais (Inglês): ['running', 'runs', 'ran', 'better', 'geese', 'children', 'is', 'are']\n","POS Tags (Inglês - simplificado): [('running', 'VBG'), ('runs', 'NNS'), ('ran', 'VBD'), ('better', 'JJR'), ('geese', 'JJ'), ('children', 'NNS'), ('is', 'VBZ'), ('are', 'VBP')]\n","Lemas (Inglês): ['run', 'run', 'run', 'good', 'geese', 'child', 'be', 'be']\n"]}]},{"cell_type":"code","source":["print(\"\\n--- Lematização em Português ---\")\n","print(\"Para português, WordNetLemmatizer do NLTK não é adequado, pois não cobre o vocabulário.\")\n","print(\"Bibliotecas como spaCy (com modelo pt) ou recursos específicos para português são necessários.\")\n","print(\"Exemplo conceitual de lemas esperados para português:\")\n","palavras_pt = [\"andando\", \"andaram\", \"anda\", \"feliz\", \"felicidade\", \"bonita\", \"bonitas\"]\n","# Lema para \"andando\", \"andaram\", \"anda\" seria \"andar\"\n","# Lema para \"feliz\", \"felicidade\" seria \"feliz\"\n","# Lema para \"bonita\", \"bonitas\" seria \"bonito\" (ou \"bonita\" dependendo da convenção/recurso)\n","\n","print(f\"Palavras originais (Português): {palavras_pt}\")\n","print(\"Lemas esperados (Conceito): ['andar', 'andar', 'andar', 'feliz', 'feliz', 'bonito', 'bonito']\")\n","\n","# Comparação com Stemming RSLP\n","# stemmer_portugues já carregado no início\n","radicais_pt = [stemmer_portugues.stem(palavra) for palavra in palavras_pt]\n","print(f\"Radicais (Stemming RSLP): {radicais_pt}\")\n","\n","# Note a diferença: stemming produz radicais que podem não ser palavras reais (\"and\", \"bonit\"),\n","# enquanto a lematização busca a forma do dicionário (\"andar\", \"feliz\", \"bonito\")."],"metadata":{"id":"FLGryQSPhquB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749751017163,"user_tz":180,"elapsed":7,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"609a6a66-0d9d-45b1-fb5f-19a213b66d97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Lematização em Português ---\n","Para português, WordNetLemmatizer do NLTK não é adequado, pois não cobre o vocabulário.\n","Bibliotecas como spaCy (com modelo pt) ou recursos específicos para português são necessários.\n","Exemplo conceitual de lemas esperados para português:\n","Palavras originais (Português): ['andando', 'andaram', 'anda', 'feliz', 'felicidade', 'bonita', 'bonitas']\n","Lemas esperados (Conceito): ['andar', 'andar', 'andar', 'feliz', 'feliz', 'bonito', 'bonito']\n","Radicais (Stemming RSLP): ['and', 'and', 'and', 'feliz', 'felic', 'bonit', 'bonit']\n"]}]},{"cell_type":"markdown","source":["**Quando usar Stemming vs. Lematização?**\n","\n","*   **Stemming:** Mais rápido, bom para tarefas onde a velocidade é importante e não é essencial que o resultado seja uma palavra real (ex: sistemas de busca simples onde você quer agrupar variações de uma palavra).\n","*   **Lematização:** Mais lento, mais preciso, bom para tarefas que exigem um entendimento mais profundo do significado da palavra ou onde a forma real da palavra é importante (ex: análise de sentimento, tradução automática, construção de vocabulário para modelos complexos).\n","\n","---"],"metadata":{"id":"5_DUnWh7h34P"}},{"cell_type":"markdown","source":["# 6. Pipeline Completo de Pré-processamento\n","\n","Vamos juntar várias dessas etapas em uma função para processar um texto de exemplo. Incorporaremos o uso de Regex na limpeza."],"metadata":{"id":"HBKbNJQxiGU2"}},{"cell_type":"code","source":["def preprocess_text(text, remove_numbers=True, remove_punctuation=True,\n","                      remove_stopwords=True, apply_stemming=False):\n","    \"\"\"\n","    Aplica um pipeline de pré-processamento a um texto usando NLTK e Regex.\n","\n","    Args:\n","        text (str): O texto de entrada.\n","        remove_numbers (bool): Se True, remove números usando Regex.\n","        remove_punctuation (bool): Se True, remove pontuação usando string.punctuation.\n","        remove_stopwords (bool): Se True, remove stop words (do NLTK).\n","        apply_stemming (bool): Se True, aplica stemming RSLP (do NLTK).\n","\n","    Returns:\n","        list: Uma lista de tokens pré-processados.\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return [] # Retorna lista vazia para entradas não string\n","\n","    # 1. Converter para minúsculas\n","    text = text.lower()\n","\n","    # 2. Remover URLs, mentions, hashtags (exemplos comuns com Regex)\n","    text = re.sub(r'https?://\\S+', '', text) # Remove URLs (http, https)\n","    text = re.sub(r'@\\w+', '', text) # Remove mentions (@usuario)\n","    text = re.sub(r'#\\w+', '', text) # Remove hashtags (#hashtag)\n","\n","\n","    # 3. Remover números (se solicitado) usando Regex\n","    if remove_numbers:\n","        text = re.sub(r'\\d+', '', text)\n","\n","    # 4. Remover pontuação (se solicitado) usando string.punctuation\n","    if remove_punctuation:\n","         text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # 5. Remover espaços em branco extras usando Regex (e strip)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    # Certificar-se de que o texto não está vazio após a limpeza inicial\n","    if not text:\n","        return []\n","\n","    # 6. Tokenização (NLTK)\n","    tokens = word_tokenize(text, language='portuguese')\n","\n","    # 7. Remover stop words (se solicitado - NLTK)\n","    if remove_stopwords:\n","        tokens = [palavra for palavra in tokens if palavra not in stop_words_portugues]\n","\n","    # 8. Aplicar stemming (se solicitado - NLTK)\n","    if apply_stemming:\n","        tokens = [stemmer_portugues.stem(palavra) for palavra in tokens]\n","\n","    # Opcional: Remover tokens vazios ou de 1 caractere que podem ter sobrado\n","    tokens = [token for token in tokens if token and len(token) > 1]\n","\n","    return tokens"],"metadata":{"id":"nUOBL-o4iLtx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exemplo de uso da função\n","texto_completo_exemplo = \"Olá mundo! Este é um teste completo para ver como o pré-processamento funciona com PALAVRAS, números 123 e pontuações diferentes! Estou testando o stemming e a lematização. Site: http://teste.com.br. Curti muito o app! #top @usuario\"\n","\n","print(f\"Texto original: {texto_completo_exemplo}\\n\")\n","\n","# --- Processamento com NLTK (sem stemming) ---\n"],"metadata":{"id":"fn9YVSRTiZCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Processamento com NLTK (com stemming) ---\n","print(\"--- Processamento com NLTK (com stemming RSLP) ---\")\n"],"metadata":{"id":"LeheBqICS7EE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. Exercícios Práticos: Raspando e Pré-processando Comentários de Apps\n","\n","Agora é a sua vez de colocar a mão na massa! Vamos aplicar o que aprendemos raspando comentários de um aplicativo da Google Play Store e pré-processando-os.\n","\n","Usaremos a biblioteca `google-play-scraper`. Você precisará instalá-la."],"metadata":{"id":"Djr2w4Wwih07"}},{"cell_type":"code","source":["!pip install google-play-scraper"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCm6qYJyimhj","executionInfo":{"status":"ok","timestamp":1749751188506,"user_tz":180,"elapsed":7746,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"ad771c2d-a09c-4048-fcdb-01086b04128b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting google-play-scraper\n","  Downloading google_play_scraper-1.2.7-py3-none-any.whl.metadata (50 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_play_scraper-1.2.7-py3-none-any.whl (28 kB)\n","Installing collected packages: google-play-scraper\n","Successfully installed google-play-scraper-1.2.7\n"]}]},{"cell_type":"markdown","source":["### **Exercício 1: Raspar Comentários de um App**\n","\n","Escolha um aplicativo na Google Play Store. Você pode encontrar o ID do aplicativo na URL da página dele. Por exemplo, a URL do WhatsApp é `https://play.google.com/store/apps/details?id=com.whatsapp`. O ID é `com.whatsapp`.\n","\n","Vamos raspar uma quantidade limitada de comentários para este exercício."],"metadata":{"id":"GbOOyuHbiraf"}},{"cell_type":"code","source":["from google_play_scraper import Sort, reviews\n","import pandas as pd\n","\n","# Substitua 'com.whatsapp' pelo ID do aplicativo que você quer raspar\n","APP_ID = 'com.google.android.youtube' # Exemplo: WhatsApp\n","NUM_REVIEWS = 20 # Quantidade de reviews a raspar (máx 200 por chamada com Sort.MOST_RELEVANT)\n","\n","all_reviews = []"],"metadata":{"id":"zE7iT-Fziqq-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A API scraper geralmente só retorna 200 por vez para a ordenação mais relevante\n","# Se quisesse mais, precisaria usar outra ordenação ou múltiplos tokens,\n","# mas para o exercício, 200 é suficiente.\n","try:\n","    result, continuation_token = reviews(\n","        APP_ID,\n","        lang='pt', # Idioma dos reviews (português)\n","        country='br', # País\n","        sort=Sort.MOST_RELEVANT, # Tipo de ordenação\n","        count=NUM_REVIEWS # Quantidade\n","    )\n","    all_reviews.extend(result)\n","\n","    print(f\"Raspagem concluída. Foram encontrados {len(all_reviews)} reviews.\")\n","\n","    if len(all_reviews) > 0:\n","        # Exibir os primeiros 5 reviews raspados\n","        print(\"\\nPrimeiros 5 reviews:\")\n","        for i, review in enumerate(all_reviews[:5]):\n","            print(f\"--- Review {i+1} ---\")\n","            print(f\"Usuário: {review.get('userName', 'N/A')}\")\n","            print(f\"Pontuação: {review.get('score', 'N/A')}\")\n","            print(f\"Data: {review.get('at', 'N/A')}\")\n","            print(f\"Conteúdo: {review.get('content', 'N/A')}\")\n","            print(\"-----------------\")\n","\n","        # Criar um DataFrame pandas com os reviews (útil para inspecionar, mas não essencial para o pré-processamento puro)\n","        reviews_df = pd.DataFrame(all_reviews)\n","        print(f\"\\nDataFrame criado com {len(reviews_df)} reviews.\")\n","        # Extrair apenas o conteúdo dos reviews para pré-processamento\n","        review_contents = reviews_df['content'].dropna().tolist() # Remover reviews sem conteúdo e converter para lista\n","\n","        print(f\"\\nTotal de conteúdos de reviews extraídos para processamento: {len(review_contents)}\")\n","    else:\n","        print(\"Nenhum review encontrado para o APP_ID especificado.\")\n","        reviews_df = pd.DataFrame()\n","        review_contents = [] # Lista vazia se não encontrar reviews\n","\n","except Exception as e:\n","    print(f\"Ocorreu um erro durante a raspagem dos reviews: {e}\")\n","    reviews_df = pd.DataFrame()\n","    review_contents = [] # Garante que review_contents é uma lista mesmo em caso de erro"],"metadata":{"id":"UKAhbRask6L1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Exercício 2: Pré-processar os Comentários Raspados**\n","\n","Agora, aplique a função `preprocess_text` que criamos para limpar os conteúdos dos reviews raspados. Você pode experimentar diferentes configurações (remover ou não números, pontuação, stop words, usar stemming)."],"metadata":{"id":"ojD6QQDblUn_"}},{"cell_type":"code","source":["# Utilize a função preprocess_text definida anteriormente\n","\n","processed_reviews = []\n","\n","if review_contents:\n","    print(\"Pré-processando os reviews com NLTK (Stemming)...\")\n","\n","    # Escolha suas opções de pré-processamento\n","    preprocess_options = {\n","        'remove_numbers': True,\n","        'remove_punctuation': True,\n","        'remove_stopwords': True,\n","        'apply_stemming': True # Experimente True e False\n","    }\n","\n","    for i, review_text in enumerate(review_contents):\n","        if (i + 1) % 50 == 0: # Apenas para mostrar o progresso a cada 50 reviews\n","            print(f\"  Processando review {i+1}/{len(review_contents)}...\")\n","\n","        # Aplica a função de pré-processamento\n","        if review_text and isinstance(review_text, str):\n","            processed_tokens = preprocess_text(review_text, **preprocess_options)\n","            processed_reviews.append(processed_tokens)\n","        else:\n","            # Adiciona uma lista vazia ou trata como preferir para reviews sem conteúdo\n","            processed_reviews.append([])\n","\n","\n","    print(\"Pré-processamento concluído!\")\n","\n","    # Exibir os tokens pré-processados dos primeiros 5 reviews (apenas se houver reviews processados)\n","    if processed_reviews:\n","        print(\"\\nPrimeiros 5 reviews pré-processados (NLTK Stemming):\")\n","        for i, tokens in enumerate(processed_reviews[:5]):\n","            print(f\"--- Review {i+1} (Tokens) ---\")\n","            print(tokens)\n","            print(\"----------------------------\")\n","    else:\n","         print(\"\\nNenhum review processado.\")\n","\n","else:\n","    print(\"Nenhum conteúdo de review para processar.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPCH_SK-lgTh","executionInfo":{"status":"ok","timestamp":1749751993060,"user_tz":180,"elapsed":24,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"f11a6b10-83d7-4cd3-eb93-e9a09408f911"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pré-processando os reviews com NLTK (Stemming)...\n","Pré-processamento concluído!\n","\n","Primeiros 5 reviews pré-processados (NLTK Stemming):\n","--- Review 1 (Tokens) ---\n","['algum', 'bug', 'coment', 'vc', 'precis', 'envi', 'vár', 'vez', 'apag', 'nad', 'proib', 'car', 'emoj', 'feliz', 'fic', 'apag', 'anúnci', 'chat', 'algum', 'long', 'curt', 'aind', 'dá', 'pra', 'aguent', 'vc', 'pul', 'vem', 'outr', 'anúnci', 'outr', 'outr', 'demor', 'além', 'diss', 'bug', 'estranh', 'avanç', 'seg', 'vc', 'tent', 'avanç', 'tr', 'parec', 'disc', 'arranh', 'dj', 'arrum']\n","----------------------------\n","--- Review 2 (Tokens) ---\n","['bom', 'aqu', 'dnv', 'problem', 'popup', 'desd', 'temp', 'sid', 'resolv', 'dá', 'ver', 'agr', 'sugest', 'legal', 'aba', 'histór', 'acrescent', 'categor', 'exempl', 'assist', 'clip', 'músic', 'ent', 'fic', 'categor', 'músic', 'assist', 'víde', 'sobr', 'jog', 'algum', 'jog', 'aí', 'fic', 'categor', 'jog', 'tip', 'tent', 'ach', 'víde', 'notíc', 'víde', 'clip', 'histór', 'extens', 'bom', 'organiz']\n","----------------------------\n","--- Review 3 (Tokens) ---\n","['sempr', 'gost', 'youtub', 'public', 'excess', 'agor', 'paus', 'víde', 'forç', 'ver', 'anúnci', 'compromet', 'usabil', 'aplic', 'dev', 'prát', 'temp', 'plataform', 'prioriz', 'lucr', 'vez', 'experi', 'usu', 'ant', 'tecnolog', 'desenvolv', 'útil', 'hoj', 'tud', 'gir', 'torn', 'monet', 'agress', 'youtub', 'continu', 'send', 'ferrament', 'vali', 'abord', 'excess', 'prejud', 'qual', 'muit', 'anúnci', 'fals']\n","----------------------------\n","--- Review 4 (Tokens) ---\n","['amo', 'us', 'youtub', 'uso', 'complet', 'tod', 'dia', 'ent', 'bast', 'últ', 'atual', 'deix', 'app', 'pouc', 'complic', 'exempl', 'histór', 'vez', 'quer', 'apag', 'histór', 'vári', 'víde', 'shot', 'assist', 'cad', 'víde', 'remov', 'sair', 'histór', 'entr', 'nov', 'recarreg', 'anteri', 'ir', 'remov', 'próx', 'vinh', 'segu', 'prát', 'rápid', 'favor', 'volt', 'ant', 'melhor']\n","----------------------------\n","--- Review 5 (Tokens) ---\n","['ire', 'resum', 'víde', 'liv', 'short', 'coment', 'post', 'anúnci', 'músic', 'mv', 'post', 'muit', 'cois', 'difer', 'ame', 'atual', 'defin', 'hor', 'post', 'prát', 'algum', 'cois', 'gost', 'anúnci', 'agor', 'aparec', 'embaix', 'víde', 'sempr', 'vou', 'ler', 'títul', 'entr', 'sit', 'gost', 'agor', 'opç', 'escrev', 'subtítul', 'víde', 'acab', 'limit', 'letr', 'rápid', 'dá', 'coloc', 'tod', 'tag', 'gost']\n","----------------------------\n"]}]},{"cell_type":"markdown","source":["### **Exercício 3: Análise Básica - Contagem de Frequência de Palavras**\n","\n","Com os tokens pré-processados, uma análise básica interessante é contar a frequência de cada palavra. Quais são as palavras mais comuns nos comentários após a limpeza? Isso pode dar insights sobre os tópicos mais mencionados nos reviews."],"metadata":{"id":"COCzCvXMlppH"}},{"cell_type":"code","source":["from collections import Counter # Já importado no início\n","import itertools # Já importado no início\n","\n","if processed_reviews:\n","    print(\"\\n--- Análise de Frequência (NLTK Stemming) ---\")\n","    # Achatar a lista de listas de tokens em uma única lista\n","    all_tokens_nltk_stem = list(itertools.chain.from_iterable(filter(None, processed_reviews)))\n","    print(f\"Total de tokens (NLTK Stemming): {len(all_tokens_nltk_stem)}\")\n","\n","    if all_tokens_nltk_stem:\n","        token_counts_nltk_stem = Counter(all_tokens_nltk_stem)\n","\n","        # Exibir os 30 tokens mais comuns\n","        print(\"\\n30 tokens mais comuns (NLTK Stemming):\")\n","        for token, count in token_counts_nltk_stem.most_common(30):\n","            print(f\"'{token}': {count}\")\n","    else:\n","         print(\"Nenhum token para contar (NLTK Stemming).\")\n","else:\n","    print(\"\\nNenhum review processado para contar a frequência.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xP0qkkZklzzV","executionInfo":{"status":"ok","timestamp":1749751999634,"user_tz":180,"elapsed":11,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"6cd21192-d61b-4fc7-90e3-93adcd05a23f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Análise de Frequência (NLTK Stemming) ---\n","Total de tokens (NLTK Stemming): 891\n","\n","30 tokens mais comuns (NLTK Stemming):\n","'víde': 45\n","'anúnci': 18\n","'bug': 15\n","'algum': 13\n","'app': 13\n","'tel': 13\n","'pra': 12\n","'fic': 11\n","'youtub': 10\n","'dá': 9\n","'outr': 9\n","'sempr': 8\n","'atual': 8\n","'ver': 7\n","'assist': 7\n","'paus': 7\n","'aplic': 7\n","'tod': 7\n","'nov': 7\n","'vez': 6\n","'pul': 6\n","'temp': 6\n","'músic': 6\n","'gost': 6\n","'us': 6\n","'aparec': 6\n","'coloc': 6\n","'simples': 6\n","'chei': 6\n","'histór': 5\n"]}]}]}