{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYufQiO2gdkXddKQQ6pfwm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Entendendo e Aplicando o XGBoost\n","\n","## 1. O que √© Boosting?\n","\n","Imagine que voc√™ tem uma tarefa complexa, como identificar se uma imagem cont√©m um gato. Em vez de tentar criar um √∫nico modelo superinteligente que acerte sempre, a ideia do **Boosting** √© construir uma equipe de modelos mais simples (chamados de \"weak learners\" ou aprendizes fracos).\n","\n","Cada novo membro da equipe √© treinado para corrigir os erros cometidos pelos membros anteriores. √â como um grupo de estudo:\n","*   O primeiro aluno tenta resolver um problema.\n","*   O segundo aluno foca nas quest√µes que o primeiro errou.\n","*   O terceiro foca nos erros combinados do primeiro e do segundo, e assim por diante.\n","\n","No final, as opini√µes de todos os membros da equipe s√£o combinadas (geralmente por uma vota√ß√£o ponderada) para tomar a decis√£o final, resultando em um \"strong learner\" (aprendiz forte).\n","\n","---\n","\n","## 2. O que √© Gradient Boosting?\n","\n","O **Gradient Boosting** √© uma t√©cnica de Boosting sofisticada. Em vez de apenas focar nos exemplos que foram classificados incorretamente (como o AdaBoost faz), o Gradient Boosting tenta corrigir os *res√≠duos* do modelo anterior.\n","\n","*   **Res√≠duo**: Para problemas de regress√£o, o res√≠duo √© simplesmente a diferen√ßa entre o valor real e o valor previsto pelo modelo anterior (erro = real - previsto).\n","*   Cada novo modelo na sequ√™ncia √© treinado para prever esses res√≠duos.\n","\n","Ao somar a previs√£o do modelo base com as previs√µes dos modelos de res√≠duos (geralmente com um pequeno \"peso\" chamado taxa de aprendizado), o modelo gradualmente se aproxima dos valores reais.\n","\n","### A M√°gica do \"Gradiente\": Uma Analogia ‚õ∞Ô∏èüö∂‚Äç‚ôÇÔ∏è\n","\n","Aqui entra a parte \"Gradiente\". Pense na fun√ß√£o de perda (loss function) do seu modelo como uma paisagem montanhosa. Seu objetivo √© encontrar o ponto mais baixo nesse vale (minimizar a perda).\n","\n","*   **Fun√ß√£o de Perda (Loss Function)**: Mede qu√£o ruim √© o seu modelo. Exemplos: Erro Quadr√°tico M√©dio (MSE) para regress√£o, Log Loss para classifica√ß√£o.\n","*   **Gradiente**: Em matem√°tica, o gradiente de uma fun√ß√£o aponta na dire√ß√£o do maior aumento da fun√ß√£o. Pense nele como a inclina√ß√£o da montanha onde voc√™ est√°. Se voc√™ quer descer (minimizar a perda), voc√™ deve ir na dire√ß√£o oposta ao gradiente (o gradiente negativo).\n","\n","**Analogia do Alpinista Cego e Preciso:**\n","\n","Imagine um alpinista (nosso algoritmo) que est√° em uma montanha (a superf√≠cie da fun√ß√£o de perda) e quer chegar ao ponto mais baixo (m√≠nimo da perda), mas ele est√° vendado.\n","\n","1.  **Primeiro Passo (Modelo Inicial)**: Ele d√° um passo inicial em alguma dire√ß√£o (faz uma primeira previs√£o, talvez a m√©dia dos valores). Ele agora est√° em algum ponto da montanha.\n","2.  **Sentindo a Inclina√ß√£o (Calculando o Gradiente)**: Para saber para onde ir, ele tateia o ch√£o ao redor para sentir a inclina√ß√£o (calcula o gradiente da fun√ß√£o de perda em rela√ß√£o √†s previs√µes atuais). O gradiente diz a ele qual √© a dire√ß√£o de subida mais √≠ngreme.\n","3.  **Descendo a Montanha (Pr√≥ximo Modelo Foca no Gradiente Negativo)**: O alpinista quer descer, ent√£o ele se move na dire√ß√£o oposta ao gradiente (gradiente negativo). No Gradient Boosting, cada novo \"weak learner\" (√°rvore de decis√£o, geralmente) √© treinado para prever esses gradientes negativos (ou algo muito pr√≥ximo a eles, chamados pseudo-res√≠duos).\n","    *   Para o Erro Quadr√°tico M√©dio (MSE), o gradiente negativo √© exatamente o res√≠duo (real - previsto). Isso torna a ideia de \"treinar nos res√≠duos\" muito intuitiva para regress√£o.\n","    *   Para outras fun√ß√µes de perda (como Log Loss na classifica√ß√£o), o gradiente negativo n√£o √© exatamente o res√≠duo, mas ainda representa a dire√ß√£o em que as previs√µes devem ser ajustadas para reduzir a perda mais rapidamente.\n","4.  **Pequenos Passos (Taxa de Aprendizado)**: O alpinista n√£o d√° um passo gigantesco na dire√ß√£o da descida, pois pode acabar passando do ponto m√≠nimo e subindo do outro lado do vale. Ele d√° pequenos passos (controlados pela \"taxa de aprendizado\" ou `learning_rate`).\n","5.  **Repetindo**: Ele repete os passos 2-4, a cada vez sentindo a nova inclina√ß√£o e dando outro pequeno passo na dire√ß√£o da descida, at√© que ele esteja satisfeito por estar perto o suficiente do fundo do vale ou at√© que um n√∫mero m√°ximo de passos seja atingido.\n","\n","Ent√£o, em ess√™ncia, o \"Gradiente\" no Gradient Boosting refere-se ao uso do gradiente da fun√ß√£o de perda para descobrir como cada novo modelo deve ajustar as previs√µes combinadas para se aproximar da solu√ß√£o √≥tima. Cada novo modelo tenta \"empurrar\" a previs√£o total na dire√ß√£o que mais reduz o erro.\n","\n","---\n","\n","## 3. XGBoost: O \"Extreme\" Gradient Boosting\n","\n","XGBoost (Extreme Gradient Boosting) √© uma implementa√ß√£o otimizada e altamente eficiente do algoritmo Gradient Boosting. Ele se tornou extremamente popular em competi√ß√µes de Machine Learning (como Kaggle) e em aplica√ß√µes industriais devido √† sua performance e velocidade.\n","\n","**Por que XGBoost? Vantagens:**\n","\n","1.  **Regulariza√ß√£o**: XGBoost inclui regulariza√ß√£o L1 (Lasso) e L2 (Ridge) para prevenir overfitting, o que o torna mais robusto que o Gradient Boosting tradicional.\n","2.  **Tratamento de Dados Faltantes (Missing Values)**: XGBoost pode lidar nativamente com dados faltantes. Durante o treinamento, ele aprende qual o melhor caminho a seguir na √°rvore quando um valor est√° faltando.\n","3.  **Paraleliza√ß√£o e Processamento Distribu√≠do**: XGBoost √© projetado para ser eficiente, permitindo o uso de m√∫ltiplos cores do processador e at√© mesmo rodar em clusters distribu√≠dos.\n","4.  **Poda de √Årvores (Tree Pruning)**: Ele usa uma abordagem mais \"profunda\" de poda (max\\_depth) e tamb√©m pode parar de dividir um n√≥ com base na melhoria do ganho (gamma) ou no n√∫mero m√≠nimo de amostras por folha (min\\_child\\_weight), tornando as √°rvores mais eficientes.\n","5.  **Valida√ß√£o Cruzada Embutida**: XGBoost pode realizar valida√ß√£o cruzada em cada itera√ß√£o do processo de boosting.\n","6.  **Early Stopping**: Pode parar o treinamento automaticamente se o desempenho em um conjunto de valida√ß√£o n√£o melhorar ap√≥s um certo n√∫mero de rodadas, economizando tempo e evitando overfitting.\n","7.  **Flexibilidade**: Altamente customiz√°vel com diversos hiperpar√¢metros para otimiza√ß√£o.\n","\n","---\n","\n","## 4. Quando Utilizar o XGBoost?\n","\n","*   **Dados Estruturados/Tabulares**: XGBoost brilha com dados em formato de tabela (como os que voc√™ encontraria em planilhas ou bancos de dados).\n","*   **Alta Performance Preditiva Necess√°ria**: Quando o objetivo principal √© obter a maior acur√°cia ou melhor m√©trica poss√≠vel.\n","*   **Problemas Complexos**: Onde as rela√ß√µes entre as features e o alvo n√£o s√£o lineares e podem ser intrincadas.\n","*   **Competi√ß√µes de Machine Learning**: √â uma escolha frequente e vencedora.\n","*   **Quando o tempo de treinamento n√£o √© a restri√ß√£o mais cr√≠tica** (embora seja r√°pido, modelos mais simples podem ser mais r√°pidos para datasets muito pequenos ou quando uma solu√ß√£o \"boa o suficiente\" √© aceit√°vel rapidamente).\n","\n","N√£o √© a melhor escolha para:\n","*   Problemas de Vis√£o Computacional ou Processamento de Linguagem Natural muito complexos (onde Deep Learning geralmente domina, embora XGBoost possa ser usado sobre features extra√≠das por modelos de DL).\n","*   Interpretabilidade extrema (modelos lineares ou √°rvores de decis√£o √∫nicas s√£o mais f√°ceis de interpretar).\n","\n","---\n","\n","## 5. Pr√©-processamento de Dados para XGBoost\n","\n","XGBoost √© relativamente robusto, mas algumas etapas de pr√©-processamento podem melhorar seu desempenho ou s√£o necess√°rias:\n","\n","1.  **Tratamento de Dados Categ√≥ricos**:\n","    *   XGBoost requer que todas as entradas sejam num√©ricas.\n","    *   **One-Hot Encoding**: √â a abordagem mais comum e geralmente recomendada para vari√°veis categ√≥ricas nominais (sem ordem intr√≠nseca). Cria novas colunas bin√°rias para cada categoria.\n","    *   **Label Encoding**: Converte categorias em n√∫meros (0, 1, 2...). Pode ser usado, mas introduz uma ordem artificial que pode n√£o ser ideal se n√£o existir na realidade. Funciona bem para vari√°veis ordinais.\n","\n","2.  **Dados Num√©ricos**:\n","    *   **Escalonamento (Scaling)**: Diferentemente de algoritmos baseados em dist√¢ncia (como KNN ou SVM), √°rvores de decis√£o (e ensembles de √°rvores como XGBoost) n√£o s√£o sens√≠veis √† escala das features. Portanto, escalonar (StandardScaler, MinMaxScaler) geralmente n√£o √© estritamente necess√°rio e, na maioria das vezes, n√£o melhora significativamente o desempenho do XGBoost. Pode at√©, em raras ocasi√µes, piorar um pouco. No entanto, n√£o costuma prejudicar.\n","\n","3.  **Dados Faltantes (Missing Values)**:\n","    *   Como mencionado, XGBoost pode lidar com `NaN` internamente. Ele aprende a melhor dire√ß√£o para dados faltantes durante o treinamento.\n","    *   No entanto, se voc√™ tiver um bom conhecimento do dom√≠nio e uma estrat√©gia de imputa√ß√£o que fa√ßa sentido para o seu problema (ex: m√©dia, mediana, valor constante, modelo preditivo para imputar), pode ser ben√©fico aplic√°-la. Testar ambas as abordagens (com e sem imputa√ß√£o expl√≠cita) √© uma boa pr√°tica.\n","\n","4.  **Feature Engineering**:\n","    *   Criar novas features a partir das existentes pode ser muito poderoso. XGBoost √© bom em encontrar intera√ß√µes, mas features bem pensadas podem ajudar.\n","\n","**Em resumo:** A principal etapa obrigat√≥ria √© converter features categ√≥ricas em num√©ricas. O tratamento de dados faltantes √© opcional (XGBoost lida bem), e o escalonamento de features num√©ricas geralmente n√£o √© necess√°rio.\n","\n"],"metadata":{"id":"ZSVMwlC-WZvz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0L2ezEGWUiR"},"outputs":[],"source":["# Importa√ß√µes b√°sicas\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# pr√©-processamento e modelagem\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","\n","# XGBoost\n","import xgboost as xgb\n","\n","print(\"XGBoost version:\", xgb.__version__)\n","\n","# Configura√ß√µes para plots\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (10, 6)"]},{"cell_type":"markdown","source":["## 6. Pr√°tica: XGBoost para Classifica√ß√£o üéØ\n","\n","Vamos usar o dataset \"Breast Cancer Wisconsin\" para um problema de classifica√ß√£o bin√°ria: prever se um tumor √© maligno (M) ou benigno (B).\n","\n","\n","\n","\n"],"metadata":{"id":"Oytby1BJoynR"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","data_cancer = load_breast_cancer()\n","X_cancer = pd.DataFrame(data_cancer.data, columns=data_cancer.feature_names)\n","y_cancer = pd.Series(data_cancer.target).map({0: 'Benigno', 1: 'Maligno'})\n","\n"],"metadata":{"id":"zbhb4Oy4pLKR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Treinando um Modelo XGBoost B√°sico (Classifica√ß√£o)\n","\n","Para problemas de classifica√ß√£o, usamos `xgb.XGBClassifier`.\n","Como todas as features j√° s√£o num√©ricas neste dataset, n√£o precisamos de One-Hot Encoding. E, como discutido, escalonamento n√£o √© estritamente necess√°rio para XGBoost."],"metadata":{"id":"zpyoLK9DrTOR"}},{"cell_type":"markdown","source":["### Avaliando o Modelo (Classifica√ß√£o)"],"metadata":{"id":"0NP5VwLbruNJ"}},{"cell_type":"markdown","source":["### Otimizando Hiperpar√¢metros com `GridSearchCV` (Classifica√ß√£o)\n","\n","XGBoost tem muitos hiperpar√¢metros. `GridSearchCV` nos ajuda a encontrar uma boa combina√ß√£o.\n","\n","Principais Hiperpar√¢metros do XGBoost:\n","*   `n_estimators`: N√∫mero de √°rvores (rodadas de boosting).\n","*   `learning_rate` (ou `eta`): Taxa de aprendizado. Reduz a contribui√ß√£o de cada √°rvore. Valores menores geralmente requerem mais `n_estimators`.\n","*   `max_depth`: Profundidade m√°xima de cada √°rvore. Controla a complexidade do modelo.\n","*   `subsample`: Fra√ß√£o de amostras usadas para treinar cada √°rvore. Ajuda a prevenir overfitting.\n","*   `colsample_bytree`: Fra√ß√£o de features usadas para treinar cada √°rvore. Ajuda a prevenir overfitting.\n","*   `gamma`: Ganho m√≠nimo para realizar uma divis√£o.\n","*   `reg_alpha` (L1) e `reg_lambda` (L2): Termos de regulariza√ß√£o."],"metadata":{"id":"zNQBgE4BsQuJ"}},{"cell_type":"markdown","source":["**Interpreta√ß√£o (Classifica√ß√£o):**\n","Com o `GridSearchCV`, buscamos uma combina√ß√£o de hiperpar√¢metros que maximize uma m√©trica de interesse (no nosso caso, `roc_auc`) na valida√ß√£o cruzada. O modelo resultante (`best_xgb_clf`) geralmente tem um desempenho melhor ou mais generaliz√°vel no conjunto de teste."],"metadata":{"id":"ikc5ydHdtgHx"}},{"cell_type":"markdown","source":["---\n","\n","## 7. Pr√°tica: XGBoost para Regress√£o üè°\n","\n","Agora, vamos usar o XGBoost para um problema de regress√£o. Usaremos o dataset ``California Housing Prices``. **O objetivo** √© prever o pre√ßo mediano das casas.\n"],"metadata":{"id":"pHYxfObnudxB"}},{"cell_type":"code","source":["# Carregando os dados de exemplo (California Housing)\n","from sklearn.datasets import fetch_california_housing\n","housing = fetch_california_housing()\n","X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n","y_housing = pd.Series(housing.target)\n","\n","print(\"Dimens√µes de X_housing:\", X_housing.shape)\n","print(\"Primeiras 5 linhas de X_housing:\")\n","print(X_housing.head())\n","print(\"\\nPrimeiras 5 linhas de y_housing (pre√ßo mediano em $100,000s):\")\n","print(y_housing.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STSd0iJmrSZx","executionInfo":{"status":"ok","timestamp":1747764898730,"user_tz":180,"elapsed":30,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"a7d16ca2-872e-4de0-8237-29ae9e2b8cc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimens√µes de X_housing: (20640, 8)\n","Primeiras 5 linhas de X_housing:\n","   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n","0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n","1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n","2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n","3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n","4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n","\n","   Longitude  \n","0    -122.23  \n","1    -122.22  \n","2    -122.24  \n","3    -122.25  \n","4    -122.25  \n","\n","Primeiras 5 linhas de y_housing (pre√ßo mediano em $100,000s):\n","0    4.526\n","1    3.585\n","2    3.521\n","3    3.413\n","4    3.422\n","dtype: float64\n"]}]},{"cell_type":"code","source":["X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(\n","    X_housing, y_housing, test_size=0.25, random_state=42\n",")\n","\n","print(\"Formato dos dados de treino:\", X_train_housing.shape, y_train_housing.shape)\n","print(\"Formato dos dados de teste:\", X_test_housing.shape, y_test_housing.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GL2dYIK_rSXJ","executionInfo":{"status":"ok","timestamp":1747764898739,"user_tz":180,"elapsed":7,"user":{"displayName":"GABRIEL MACHADO LUNARDI","userId":"06253869185260362260"}},"outputId":"9ac16b9a-cbed-4ad9-9f0c-a4fb6232cb27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Formato dos dados de treino: (15480, 8) (15480,)\n","Formato dos dados de teste: (5160, 8) (5160,)\n"]}]},{"cell_type":"markdown","source":["### Treinando um Modelo XGBoost B√°sico (Regress√£o)\n","\n","Para problemas de regress√£o, usamos `xgb.XGBRegressor`."],"metadata":{"id":"2LG1iQrTvDgI"}},{"cell_type":"markdown","source":["### Avaliando o Modelo (Regress√£o)\n","\n","M√©tricas comuns para regress√£o:\n","*   **Mean Squared Error (MSE)**: M√©dia dos erros ao quadrado. Penaliza erros grandes.\n","*   **Root Mean Squared Error (RMSE)**: Raiz quadrada do MSE. Est√° na mesma unidade da vari√°vel alvo.\n","*   **Mean Absolute Error (MAE)**: M√©dia dos erros absolutos. Mais robusto a outliers.\n","*   **R-squared (R¬≤)**: Coeficiente de determina√ß√£o. Propor√ß√£o da vari√¢ncia na vari√°vel dependente que √© previs√≠vel a partir das vari√°veis independentes. Varia de -‚àû a 1 (melhor)."],"metadata":{"id":"KfcZN4b0vMvZ"}},{"cell_type":"code","source":["# Fazer previs√µes no conjunto de teste\n","y_pred_housing_basic = xgb_reg_basic.predict(X_test_housing)\n","\n","# Avaliar\n","mse_basic = mean_squared_error(y_test_housing, y_pred_housing_basic)\n","rmse_basic = np.sqrt(mse_basic)\n","mae_basic = mean_absolute_error(y_test_housing, y_pred_housing_basic)\n","r2_basic = r2_score(y_test_housing, y_pred_housing_basic)\n","\n","print(\"--- M√©tricas do XGBRegressor B√°sico ---\")\n","print(f\"Mean Squared Error (MSE): {mse_basic:.4f}\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse_basic:.4f}\")\n","print(f\"Mean Absolute Error (MAE): {mae_basic:.4f}\")\n","print(f\"R-squared (R¬≤): {r2_basic:.4f}\")\n","\n","# Plot de Previsto vs Real\n","plt.figure(figsize=(8, 8))\n","plt.scatter(y_test_housing, y_pred_housing_basic, alpha=0.5)\n","plt.plot([y_test_housing.min(), y_test_housing.max()], [y_test_housing.min(), y_test_housing.max()], 'k--', lw=2)\n","plt.xlabel('Valores Reais')\n","plt.ylabel('Valores Previstos')\n","plt.title('Regress√£o XGBoost (B√°sico): Reais vs. Previstos')\n","plt.show()"],"metadata":{"id":"DyvP4XHYsNZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Otimizando Hiperpar√¢metros com `GridSearchCV` (Regress√£o)"],"metadata":{"id":"fSopZncfvs1B"}},{"cell_type":"code","source":["# Avaliar o melhor modelo encontrado pelo GridSearchCV\n","y_pred_housing_tuned = best_xgb_reg.predict(X_test_housing)\n","\n","# Avaliar\n","mse_tuned = mean_squared_error(y_test_housing, y_pred_housing_tuned)\n","rmse_tuned = np.sqrt(mse_tuned)\n","mae_tuned = mean_absolute_error(y_test_housing, y_pred_housing_tuned)\n","r2_tuned = r2_score(y_test_housing, y_pred_housing_tuned)\n","\n","print(\"\\n--- M√©tricas do XGBRegressor Otimizado ---\")\n","print(f\"Mean Squared Error (MSE): {mse_tuned:.4f}\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse_tuned:.4f}\")\n","print(f\"Mean Absolute Error (MAE): {mae_tuned:.4f}\")\n","print(f\"R-squared (R¬≤): {r2_tuned:.4f}\")\n","\n","# Comparar com o b√°sico\n","print(\"\\n--- Melhorias ---\")\n","print(f\"Redu√ß√£o no RMSE: {rmse_basic - rmse_tuned:.4f} (Quanto menor, melhor)\")\n","print(f\"Aumento no R¬≤: {r2_tuned - r2_basic:.4f} (Quanto maior, melhor)\")\n","\n","\n","# Plot de Previsto vs Real para o modelo otimizado\n","plt.figure(figsize=(8, 8))\n","plt.scatter(y_test_housing, y_pred_housing_tuned, alpha=0.5, color='green')\n","plt.plot([y_test_housing.min(), y_test_housing.max()], [y_test_housing.min(), y_test_housing.max()], 'k--', lw=2)\n","plt.xlabel('Valores Reais')\n","plt.ylabel('Valores Previstos')\n","plt.title('Regress√£o XGBoost (Otimizado): Reais vs. Previstos')\n","plt.show()"],"metadata":{"id":"shgI4galsNU_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Interpreta√ß√£o (Regress√£o):**\n","Similar √† classifica√ß√£o, o `GridSearchCV` ajuda a encontrar hiperpar√¢metros que minimizam o erro (neste caso, o MSE negativo √© maximizado, o que equivale a minimizar o MSE) na valida√ß√£o cruzada. O modelo otimizado geralmente apresenta m√©tricas de erro menores (MSE, RMSE, MAE) e um R¬≤ mais alto no conjunto de teste.\n","\n","---"],"metadata":{"id":"oHcvmVYwwOVp"}},{"cell_type":"markdown","source":["\n","\n","\n","\n","\n","## 8. Conclus√£o\n","\n","*   **Gradient Boosting** √© uma t√©cnica poderosa que constr√≥i modelos sequencialmente, onde cada novo modelo corrige os erros (ou mais precisamente, os gradientes negativos da fun√ß√£o de perda) do anterior.\n","*   A analogia do **alpinista cego** ajuda a entender como o \"gradiente\" guia o algoritmo para minimizar a fun√ß√£o de perda.\n","*   **XGBoost** √© uma implementa√ß√£o otimizada do Gradient Boosting, oferecendo alta performance, regulariza√ß√£o, e bom tratamento de dados.\n","*   √â essencial para XGBoost que **features categ√≥ricas sejam convertidas para formato num√©rico**. Escalonamento de features num√©ricas √© geralmente opcional.\n","*   A otimiza√ß√£o de hiperpar√¢metros com ferramentas como `GridSearchCV` √© fundamental para extrair o m√°ximo desempenho do XGBoost.\n","*   XGBoost √© vers√°til, aplic√°vel tanto a problemas de **classifica√ß√£o** (`XGBClassifier`) quanto de **regress√£o** (`XGBRegressor`).\n","\n","---\n","\n","## 9. Exerc√≠cios Propostos\n","\n","1.  **Explorando Outros Hiperpar√¢metros (Classifica√ß√£o):**\n","    *   No problema de classifica√ß√£o do c√¢ncer de mama, adicione os hiperpar√¢metros `gamma` e `reg_alpha` (regulariza√ß√£o L1) ao `param_grid_clf` do `GridSearchCV`.\n","    *   Quais s√£o os melhores valores encontrados para esses novos par√¢metros?\n","    *   Houve melhora significativa no desempenho do modelo no conjunto de teste em compara√ß√£o com o modelo otimizado anteriormente?\n","    *   **Dica**: `gamma` pode variar de 0 a valores maiores (ex: 0, 0.1, 0.5, 1). `reg_alpha` tamb√©m (ex: 0, 0.01, 0.1, 1).\n","\n","2.  **XGBoost com Pr√©-processamento de Categ√≥ricas (Novo Dataset - Titanic):**\n","    *   Utilize o dataset \"Titanic\" (cl√°ssico para classifica√ß√£o). Voc√™ pode carreg√°-lo facilmente com `sns.load_dataset('titanic')`.\n","    *   O objetivo √© prever a coluna `survived`.\n","    *   **Desafios:**\n","        *   Identifique as colunas num√©ricas e categ√≥ricas.\n","        *   Trate os valores faltantes (ex: imputar idade com a mediana, `embarked` com a moda).\n","        *   Aplique One-Hot Encoding nas features categ√≥ricas relevantes (ex: `sex`, `embarked`, `pclass` se tratada como categ√≥rica). Use `ColumnTransformer` e `Pipeline` para organizar o pr√©-processamento.\n","        *   Divida os dados em treino e teste.\n","        *   Treine um `XGBClassifier` e avalie seu desempenho.\n","        *   (B√¥nus) Tente otimizar os hiperpar√¢metros com `GridSearchCV`.\n","    *   Como o XGBoost se saiu neste dataset com mais pr√©-processamento envolvido?"],"metadata":{"id":"tvm9Dy4lpCPp"}}]}