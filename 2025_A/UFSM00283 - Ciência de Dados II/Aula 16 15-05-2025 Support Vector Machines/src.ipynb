{"cells":[{"cell_type":"markdown","metadata":{"id":"6F9xn3yKE5v3"},"source":["# O SVM (Support Vector Machine)\n","\n","Para iniciar a explicação, vamos retomar visualmente o dataset Iris. Lembre-se que as classes (targets) são representadas pelos valores 0, 1 e 2.  \n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:736/1*H2UmG5L1I5bzFCW006N5Ag.png\" style='width: 600px;' />\n","\n","- Utilizando o dataset iris, podemos utilizar apenas a coluna petal width (cm) e visualizar a seguinte classificação dos dados:\n","<img src=\"https://drive.google.com/uc?id=1uEDHjpB-etkcNadcNYbBZqZeOBrIhCLi\" style='width: 600px;' />\n","<br>\n","- Se tivermos um novo ponto, como o mostrado em vermelho, automaticamente já vamos pensar que esse ponto possui a mesma classe dos pontos em roxo:\n","<img src=\"https://drive.google.com/uc?id=1iYYZoHp4B9Suh6sI2H7G6G2WI6M-RKFP\" style='width: 600px;' />\n","<br>\n","- Da mesma forma, tendemos a pensar que esse novo ponto deve ser classificado como amarelo:\n","<img src=\"https://drive.google.com/uc?id=1dfMuwPa6-r7CW9btaWnkHn3hVEDykKw6\" style='width: 600px;' />\n","<br>\n","- E é uma lógica muito parecida com essa que temos no SVM\n","<br>\n","- Podemos basicamente pegar os pontos que estão mais no extremo dos dados (**os pontos de duas classes diferentes que estão mais próximos entre si**) e usar esses pontos para determinar um <font color='red'>**hiperplano**</font> capaz de separar essas classes\n","<img src=\"https://drive.google.com/uc?id=1KoDBoT2Lcm31A-ebAuWLgilSEEeY4OlW\" style='width: 600px;' />\n","- **Quanto maior a margem, melhor!**\n","<br><br>\n","- Um fato importante é que ele **pode ser utilizado tanto em dados linearmente separáveis** (como mostrado acima) **como em dados que não são linearmente separáveis** (que são a maioria dos nossos dados)\n","    - Ele faz isso elevando as dimensões dos dados\n","    - O SVM vai usar as funções de kernel e no próprio algoritmo ele vai buscar a melhor dimensão capaz de encontrar essa reta\n","    - Só que ele não faz efetivamente essas transformações, ele só vai calcular a relação entre os pontos considerando que eles estão nessas dimensões. Isso é chamado de **\"Truque do Kernel\"**\n","<br>\n","<img src=\"https://drive.google.com/uc?id=1DpLpHou1tc4UayxKFglqb4I48EhjoktX\" style='width: 1000px;' />\n","<br>\n","- E por mais que a gente tenha visto em apenas 1D, ele funciona para qualquer dimensão dos dados\n","<img src=\"https://drive.google.com/uc?id=17U2EPYj1pmbkXFimoLLP1mDWN0uiW3kP\" style='width: 400px;' />\n","<br>\n","- **É um algoritmo muito poderoso porém pode demorar muito tempo para treinar os dados**"],"id":"6F9xn3yKE5v3"},{"cell_type":"markdown","source":["## Vamos fazer um treinamento rápido e simples para visualizar os vetores de suporte\n","\n","- Para fins didáticos, vamos utilizar apenas duas características: o comprimento da pétala e a largura da pétala.\n","- Vamos considerar, também, apenas duas classes (flores): 0 e 1"],"metadata":{"id":"hCZ-f8rNNdRy"},"id":"hCZ-f8rNNdRy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgRG_Fb8E5v6"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","X,y = load_iris(return_X_y = True, as_frame=True)"],"id":"CgRG_Fb8E5v6"},{"cell_type":"markdown","metadata":{"id":"WOQGJQytE5v9"},"source":["O SVC (Support Vector Classifier) é uma implementação do algoritmo SVM (Support Vector Machine) para problemas de classificação, disponível na biblioteca scikit-learn do Python: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n","\n","Por padrão, o SVM foi originalmente concebido para problemas de classificação binária. Como limitados o nosso y a duas classes (flores) apenas, é assim que o algoritmo funcionará."],"id":"WOQGJQytE5v9"},{"cell_type":"markdown","metadata":{"id":"NfqJO3ztE5v-"},"source":["- Para traçar a reta, podemos considerar que a equação da reta será dada por:\n","    - y = ax + b\n","- Para considerar os dois coeficientes, também podemos escrever como:\n","    - w1.X_train[0] + w2.X_train[1] + w0 = 0\n","    - w1.X_train['petal width (cm)'] + w2.X_train['petal length (cm)'] + w0 = 0\n","    - Sendo x = X_train['petal width (cm)'] e y = X_train['petal length (cm)']:\n","        - w1.x + w2.y + w0 = 0\n","        - w2.y = -w1.x - w0\n","        - y = (-w1.x - w0)/w2"],"id":"NfqJO3ztE5v-"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xckwXeDBE5v-"},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","ax.scatter(X_train['petal length (cm)'], X_train['petal width (cm)'],c=y_train,s=60)\n","\n","x = np.linspace(1,5,100)\n","y = (-w1*x-w0)/w2\n","ax.plot(x,y,'r')\n","\n","ax.set(ylim=(0,1.7))\n","\n","plt.show()"],"id":"xckwXeDBE5v-"},{"cell_type":"markdown","source":["Além disso, também podemos ver quais foram os vetores de suporte utilizados para traçar essa reta."],"metadata":{"id":"GijWby_4MFlK"},"id":"GijWby_4MFlK"},{"cell_type":"markdown","source":["Pegando os valores de x"],"metadata":{"id":"j5nQ9ZTHMKOz"},"id":"j5nQ9ZTHMKOz"},{"cell_type":"markdown","source":["E os valores de y"],"metadata":{"id":"3cJJs2H2MRmr"},"id":"3cJJs2H2MRmr"},{"cell_type":"markdown","source":["Visualizando esses pontos"],"metadata":{"id":"N9cgykU8MTrb"},"id":"N9cgykU8MTrb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZxVhBRyxE5v-"},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","ax.scatter(X_train['petal length (cm)'], X_train['petal width (cm)'],c=y_train,s=60)\n","\n","x = np.linspace(1,5,100)\n","y = (-w1*x-w0)/w2\n","ax.plot(x,y,'r')\n","y2 = (+1-w1*x-w0)/w2\n","ax.plot(x,y2,'--r')\n","y3 = (-1-w1*x-w0)/w2\n","ax.plot(x,y3,'--r')\n","\n","ax.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],c='r')\n","\n","ax.set(ylim=(0,1.7))\n","\n","plt.show()"],"id":"ZxVhBRyxE5v-"},{"cell_type":"markdown","metadata":{"id":"imnURNOVE5v-"},"source":["##Vamos agora fazer um pipeline completo, até a geração das métricas de avaliação\n","\n","Quando aplicado a problemas com mais de duas classes (como no dataset Iris com 3 classes), o scikit-learn usa uma destas abordagens:\n","\n","1. One-vs-Rest (OvR) - Padrão no SVC\n","  - Treina N classificadores (onde N = número de classes)\n","  - Cada classificador distingue uma classe vs todas as outras\n","2. One-vs-One (OvO) - geralmente mais preciso para dados balanceados\n","\n","\n","**Pré-processamento geralmente requerido pelo SVM:**\n","\n","1. Dados Ausentes: Impute (preencha) ou remova.\n","2. Escalonamento: Padronize (StandardScaler) ou normalize (MinMaxScaler) as features.\n","3. Outliers: Remova, transforme ou deixe como estão (avaliar o impacto).\n","4. Categóricas: Converta para numéricas (One-Hot Encoding ou Label Encoding).\n","5. Balanceamento de Classes (Se necessário): Oversampling, undersampling ou ajuste de pesos no modelo.\n","\n","\n"],"id":"imnURNOVE5v-"},{"cell_type":"markdown","source":["Criando um classificador SVM com kernel RBF (Radial Basis Function)\n","\n","- O **kernel RBF** é uma função que define como o SVM vai mapear os dados para um espaço de maior dimensão para encontrar um hiperplano que melhor separa as classes."],"metadata":{"id":"urbIJw7S7rFM"},"id":"urbIJw7S7rFM"},{"cell_type":"markdown","source":["<img src=\"https://cdn.botpenguin.com/assets/website/Screenshot_2024_02_27_at_3_29_53_PM_1_617d837c55.webp\" />\n","\n","\n","\n","\n"],"metadata":{"id":"vf1y9yurARGd"},"id":"vf1y9yurARGd"},{"cell_type":"markdown","source":["- C=1.0: **Parâmetro de regularização.** Controla o *trade-off* entre classificar corretamente os dados de treino e ter uma margem de decisão suave. Valores menores de C permitem mais erros de classificação no treino, mas podem generalizar melhor para dados não vistos. Valores maiores de C penalizam mais erros de classificação no treino, o que pode levar a overfitting."],"metadata":{"id":"Td6_JKHcDMc_"},"id":"Td6_JKHcDMc_"},{"cell_type":"markdown","source":["# Extra\n","\n","Algo comum em pipelines de ciência de dados é a experimentação. Isto porque várias dúvidas surgem e uma delas é qual é o melhor valor dos hiperparâmetros dos algoritmos.\n","\n","Nesse sentido, O GridSearchCV (Grid Search Cross-Validation) é uma ferramenta  do scikit-learn que automatiza o processo de testar diferentes combinações de hiperparâmetros e, em seguida, seleciona a combinação que oferece o melhor desempenho de acordo com uma métrica de avaliação especificada.\n","\n","Façamos isso para o algoritmo SVM que vimos nesta aula."],"metadata":{"id":"Xg_RbcZWEfO2"},"id":"Xg_RbcZWEfO2"},{"cell_type":"markdown","source":["**Exercício 1:** Altere a quantidade de instâncias no conjunto de teste para 20%. O que acontece com as métricas? Por quê?\n","\n","**Exercício 2:** Retome aulas anteriores sobre pré-processamento e aplique min-max ou StdScaler, conforme a natureza de cada feature. Utilize 33% dos dados no conjunto de teste.\n","\n","**Exercício 3:** Existe uma versão do SVM para problemas de regressão, o SVR que funciona de forma semelhante e com os mesmos parâmetros, com diferença para o erro (e não acurácia). Escolha um dataset cuja variável alvo seja um número e aplique este algoritmo, utilizando o GridSearchCV."],"metadata":{"id":"f2ja49ZoG3Vn"},"id":"f2ja49ZoG3Vn"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}